1
00:00:00,000 --> 00:00:02,054
為甚麼我們要用 deep 呢？

2
00:00:02,054 --> 00:00:07,307
接下來就是要講我們用 deep 的理由

3
00:00:09,513 --> 00:00:13,871
那要用 deep 的理由一點都不是新鮮的東西

4
00:00:14,148 --> 00:00:15,925
非常早以前就有人知道了

5
00:00:15,925 --> 00:00:19,404
我記得我昨天跟助教講說，我明天要來

6
00:00:19,404 --> 00:00:22,799
證一下 deep 是不是比較好的，助教就說

7
00:00:22,799 --> 00:00:26,292
這個不是三年前就講過一模一樣的東西了嗎？

8
00:00:26,292 --> 00:00:30,391
三年前聽你上課的時候，第一堂就講了類似的東西

9
00:00:30,391 --> 00:00:32,819
但這跟三年前講的東西是不一樣的

10
00:00:32,819 --> 00:00:36,137
三年前講的時候，我們只有直覺而已

11
00:00:36,137 --> 00:00:39,223
但是，現在在三年後，deep learning 發展得很快

12
00:00:39,284 --> 00:00:42,905
不只有了直覺，還有了更多的理論的基礎

13
00:00:42,905 --> 00:00:45,870
所以，跟之前講的東西是不一樣的

14
00:00:46,860 --> 00:00:48,669
那其實 deep 比較好這一件事情

15
00:00:48,669 --> 00:00:51,404
在很早以前就有這樣子的猜想

16
00:00:51,404 --> 00:00:53,212
舉例來說，你看那個

17
00:00:53,212 --> 00:00:57,531
Yann Lecun 他們在 09 年的時候，寫的時候

18
00:00:57,531 --> 00:01:00,110
為甚麼我們要用 deep network 的時候

19
00:01:00,110 --> 00:01:02,597
裡面就有各式各樣的猜想告訴我們說

20
00:01:02,597 --> 00:01:04,691
deep 是比較好的

21
00:01:04,691 --> 00:01:08,072
只是在過去，並沒有太多的理論的證明

22
00:01:08,072 --> 00:01:11,049
但是，現在已經有了很多理論的證明了

23
00:01:11,049 --> 00:01:14,484
那我把相關理論的 paper 都附在這個投影片的後面

24
00:01:14,484 --> 00:01:15,919
你有興趣的話再參考

25
00:01:15,919 --> 00:01:18,065
今天所講的內容並不是

26
00:01:18,065 --> 00:01:20,486
base on 某一篇 paper 的理論講的

27
00:01:20,486 --> 00:01:23,116
你也知道上課嘛，上課要講的東西是

28
00:01:23,116 --> 00:01:25,934
還是希望你們可以聽得懂這樣子

29
00:01:25,934 --> 00:01:29,857
所以稍微做了一些簡化，希望大家是可以聽得懂的

30
00:01:29,857 --> 00:01:33,561
那為甚麼我們要用 deep，我們都知道說

31
00:01:33,561 --> 00:01:37,919
雖然 shallow 的 network 可以表示任何的 function

32
00:01:37,931 --> 00:01:39,586
你用 shallow network 去 fit

33
00:01:39,586 --> 00:01:43,833
任何的 L lipschitz function 到任何的精準度

34
00:01:43,833 --> 00:01:45,183
只要 neuron 夠多

35
00:01:45,183 --> 00:01:47,256
但是，沒有告訴你的是

36
00:01:47,256 --> 00:01:49,191
今天所謂的 neuron 夠多

37
00:01:49,191 --> 00:01:52,575
到底要多到甚麼樣的地步

38
00:01:52,575 --> 00:01:56,974
那我們剛才已經講說，其實你 neuron 的數目是 [L 除以 ε] {L / ε}

39
00:01:56,974 --> 00:02:00,060
neuron 的數目是 [L 除以 ε] {L / ε}

40
00:02:00,060 --> 00:02:04,064
接下來要看的就是說，如果我們用 deep 的

41
00:02:04,064 --> 00:02:08,799
加一個 你說的對，是兩倍

42
00:02:08,799 --> 00:02:11,325
那為甚麼剛才沒有加兩倍呢？是因為

43
00:02:11,325 --> 00:02:15,577
其實你永遠可以想到更好的方法來 fit 這個東西

44
00:02:15,577 --> 00:02:17,532
也就是說，我剛才是用比較笨的方法

45
00:02:17,532 --> 00:02:20,224
是用兩個 neuron 才知道一條直線

46
00:02:20,224 --> 00:02:21,902
其實你仔細想一想，應該用

47
00:02:21,902 --> 00:02:24,052
一個 neuron 就可以知道一條直線了

48
00:02:24,052 --> 00:02:27,532
所以，比較好的寫法就是放一個 big O 這樣子

49
00:02:27,532 --> 00:02:29,911
[big O of L 除以 ε] {O(L/ε)} ，意思就是說前面

50
00:02:29,911 --> 00:02:31,818
不知道 O 是什麼的人，也就是說

51
00:02:31,818 --> 00:02:36,087
這前面有一個常數，我們不太確定這個常數是什麼

52
00:02:36,087 --> 00:02:43,159
那我們等一下就會講說，deep 可不可以做得比這個好

53
00:02:43,159 --> 00:02:45,057
可不可以做得比這個好

54
00:02:47,713 --> 00:02:50,257
但是在講理論的部分之前

55
00:02:50,257 --> 00:02:53,738
我們先回顧一下，在過去十年來

56
00:02:53,738 --> 00:02:55,033
人們是怎麼講的

57
00:02:55,033 --> 00:02:58,702
過去的時候，通常不是理論的證明，就是舉個例子

58
00:02:58,702 --> 00:03:01,798
舉個例子，就是打個比方說

59
00:03:01,798 --> 00:03:05,037
為什麼我們需要 deep，舉例來說我們在寫程式的時候

60
00:03:05,037 --> 00:03:07,886
其實你知道，任何的演算法

61
00:03:07,886 --> 00:03:10,570
都可以用兩行程式就 implement

62
00:03:11,743 --> 00:03:15,521
就好像說，將大象塞進冰箱就只要三個步驟一樣

63
00:03:15,521 --> 00:03:18,239
其實任何演算法都可以用兩行程式 implement

64
00:03:18,239 --> 00:03:21,867
怎麼 implement 呢？舉例來說，假設今天演算法

65
00:03:21,867 --> 00:03:23,471
是 sorting 的演算法

66
00:03:23,471 --> 00:03:28,634
你就把所有 sort 前的字串通通找出來

67
00:03:28,634 --> 00:03:30,293
窮舉所有可能 input

68
00:03:30,293 --> 00:03:33,647
把每一個 output 的可能通通事先算好

69
00:03:34,965 --> 00:03:36,121
當作它的 value

70
00:03:36,121 --> 00:03:37,528
所有可能 input 都是 key

71
00:03:37,528 --> 00:03:39,697
所有可對應的 output 就是 value

72
00:03:39,697 --> 00:03:42,133
A 是一個數字的 sequence

73
00:03:42,133 --> 00:03:43,688
[A prime] {A'} 是他 sorting 好的結果

74
00:03:43,688 --> 00:03:45,889
假設你要 implement 一個 sorting 的演算法的話

75
00:03:45,889 --> 00:03:48,446
存一個巨大的 table，把它存好

76
00:03:48,446 --> 00:03:50,245
接下來程式就這樣寫

77
00:03:50,245 --> 00:03:52,605
input 一個 sequence 叫做 K

78
00:03:52,605 --> 00:03:56,713
然後，第一行就是查表，call 一個 function 叫做

79
00:03:56,713 --> 00:03:59,588
match key，看看這邊有沒有那個 key 出現

80
00:03:59,588 --> 00:04:01,397
然後，把那個 key 的

81
00:04:01,397 --> 00:04:04,250
落在第幾個 row 的那個 row 的 number

82
00:04:04,250 --> 00:04:05,922
row 的 id 回傳回來

83
00:04:05,922 --> 00:04:08,563
然後再看那個 row 的 id 對應到的 value 是什麼

84
00:04:08,563 --> 00:04:09,691
把那個 value 吐出去

85
00:04:09,691 --> 00:04:12,609
你就 implement 完任何演算法了，就這樣

86
00:04:12,609 --> 00:04:15,242
所以我們知道說，其實任何演算法

87
00:04:15,242 --> 00:04:17,397
都可以用兩行程式來 implement

88
00:04:17,397 --> 00:04:20,168
但是，沒有人用這樣的方法

89
00:04:20,168 --> 00:04:22,047
來 implement sorting 這樣

90
00:04:22,047 --> 00:04:24,964
其實，像這樣子的方法

91
00:04:24,964 --> 00:04:28,526
你仔細想想看，他有點像是 SVM 加上 kernel

92
00:04:28,606 --> 00:04:30,938
對不對？如果大家熟悉 SVM 的話

93
00:04:30,938 --> 00:04:33,167
SVM 加上 kernel 是怎麼 implement 的

94
00:04:33,167 --> 00:04:35,081
你有一筆 data, x 進來

95
00:04:35,081 --> 00:04:36,952
然後這邊的 n 代表說

96
00:04:36,952 --> 00:04:38,610
所有的 training data

97
00:04:38,610 --> 00:04:40,884
你的 training data 有 n 筆，從 [x one] {x^1} 到 [x n] {x^n}

98
00:04:40,884 --> 00:04:43,230
你把每一筆 data

99
00:04:43,230 --> 00:04:45,906
input 的 data, x 跟每一筆 training data, [x n] {x^n}

100
00:04:45,906 --> 00:04:48,051
都去計算他們的相似度

101
00:04:48,051 --> 00:04:51,296
[K of x n 跟 x] {K(x^n, x)} 代表他們之間的相似度

102
00:04:51,296 --> 00:04:55,274
只是不同的 kernel 代表你使用了不同的相似度

103
00:04:55,274 --> 00:04:58,397
x 跟 [x n] {x^n} 去計算了相似度之後

104
00:04:58,397 --> 00:05:02,546
然後再乘上 [alpha n] {αn}，最後就得到他的 output

105
00:05:02,546 --> 00:05:04,921
所以，計算相似度這件事啊

106
00:05:04,921 --> 00:05:07,369
其實就是 key matching

107
00:05:07,369 --> 00:05:09,441
乘上 [alpha n] {αn}，再輸出

108
00:05:09,441 --> 00:05:13,052
其實就是你得到 row id 以後，把你的結果輸出來

109
00:05:13,052 --> 00:05:14,780
只是如果 SVM with kernel 的話

110
00:05:14,780 --> 00:05:17,064
他不是只抽一個 key 而已

111
00:05:17,064 --> 00:05:20,699
他會算跟不同的 key 有不同的 match 程度

112
00:05:20,699 --> 00:05:23,161
然後把你的 value 做 weighted sum

113
00:05:23,161 --> 00:05:26,519
但是，其實 SVM 加上 kernel 他在本質上

114
00:05:26,519 --> 00:05:30,413
他在做這個，解這個 machine learning 的問題的時候

115
00:05:30,413 --> 00:05:31,759
他在描述一個 function 的時候

116
00:05:31,759 --> 00:05:34,562
就很像是這種兩行的演算法

117
00:05:34,562 --> 00:05:37,157
但是我們知道說，你不會用

118
00:05:37,157 --> 00:05:39,520
兩行 code 來 implement algorithm

119
00:05:39,520 --> 00:05:42,209
你會用好多個 step 來 implement algorithm

120
00:05:42,209 --> 00:05:45,061
為什麼？因為這樣是比較有效率的方式

121
00:05:45,061 --> 00:05:48,436
因為你並不需要存一個碩大無朋的 table

122
00:05:48,436 --> 00:05:51,297
那這個是一個比喻

123
00:05:51,297 --> 00:05:54,987
還有另一個比喻是用這個 circuit

124
00:05:54,987 --> 00:05:58,327
來做比喻，假設你是電機系的同學的話

125
00:05:58,327 --> 00:06:02,686
一定修過邏輯電路設計，這個是必修

126
00:06:02,686 --> 00:06:04,979
那在邏輯電路裡面，我們學到說

127
00:06:04,979 --> 00:06:09,260
所有的邏輯電路都是由 gate 所構成的

128
00:06:09,260 --> 00:06:13,568
就好像 neuron，neural network 都是由 neuron 所構成的

129
00:06:13,568 --> 00:06:16,572
那我們知道只要兩層的邏輯閘

130
00:06:16,572 --> 00:06:18,544
就可以組成任何的 boolean function

131
00:06:18,544 --> 00:06:19,619
我們都知道說

132
00:06:19,619 --> 00:06:21,358
只要一個 hidden layer 的 network

133
00:06:21,358 --> 00:06:23,180
一個 hidden layer network 其實也是兩層

134
00:06:23,180 --> 00:06:25,444
hidden layer 跟 output layer 也是兩層

135
00:06:25,444 --> 00:06:28,243
可以表示任何的 continuous function

136
00:06:28,243 --> 00:06:31,777
但是你不會用兩層的邏輯閘來設計電路

137
00:06:31,777 --> 00:06:33,958
因為設計出來的電路太過龐大

138
00:06:33,958 --> 00:06:36,339
如果你有用 hierarchical 的結構的話

139
00:06:36,339 --> 00:06:39,188
你有用 multi-layer 的話，設計出來的電路

140
00:06:39,188 --> 00:06:40,805
會是比較精簡的

141
00:06:40,805 --> 00:06:44,695
所以，如果你用 deep 的結構來設計電路

142
00:06:44,695 --> 00:06:47,134
你的電路裡面，邏輯閘是有好幾層的

143
00:06:47,134 --> 00:06:49,889
那你設計出來的電路會比較精簡

144
00:06:49,889 --> 00:06:52,961
你要做同樣的事情，只需要比較少的 gate

145
00:06:52,961 --> 00:06:55,467
那對 network 來說，在過去就是

146
00:06:55,467 --> 00:06:58,160
也不怎麼證明，就打個比方這樣子

147
00:06:58,160 --> 00:07:02,737
就說 A 是這個樣子，B 應該也是一樣吧

148
00:07:02,737 --> 00:07:04,820
如果有很多個 layer 的話

149
00:07:04,833 --> 00:07:07,724
就有很多個很多個 layer 的 neuron

150
00:07:07,724 --> 00:07:09,983
那你要描述一個 function 的時候，應該比較容易

151
00:07:09,983 --> 00:07:13,077
所以你只要用 deep 的架構，只需要比較少的 neuron

152
00:07:13,077 --> 00:07:15,394
就可以描述同樣的 function

153
00:07:15,394 --> 00:07:16,999
那這樣可能會被反駁說

154
00:07:16,999 --> 00:07:21,143
A 很像 B，並不代表他們的性質就是一樣的

155
00:07:21,629 --> 00:07:25,958
但他們確實是很像的，理論可能是相同的

156
00:07:25,958 --> 00:07:29,085
那下一頁這個例子

157
00:07:29,085 --> 00:07:32,839
我打算把它略過，你可以自己看一下

158
00:07:32,839 --> 00:07:34,855
當我們今天講到 network 的架構

159
00:07:34,855 --> 00:07:37,301
把 network 的架構跟 deep 的 network

160
00:07:37,301 --> 00:07:42,865
勾在一起的時候，通常就舉這個例子，告訴你說

161
00:07:42,865 --> 00:07:44,565
在電路裡面

162
00:07:44,565 --> 00:07:47,850
如果你要 implement 一個叫 parity check 的電路

163
00:07:47,850 --> 00:07:51,670
你可以用 shallow 的 network 來 implement

164
00:07:51,670 --> 00:07:54,398
但是，如果你 implement 的長度是 d 的話

165
00:07:54,398 --> 00:07:57,743
你要 [big O of 二的 d 次方] {O(2^d)} 的 gate

166
00:07:57,743 --> 00:08:02,090
但是，假設你今天用一個 deep 的架構來 implement

167
00:08:02,090 --> 00:08:04,818
實際上的細節就不管了，反正就弄成這個樣子

168
00:08:04,818 --> 00:08:07,735
deep 的架構，那其實你只需要

169
00:08:07,735 --> 00:08:11,371
[big O of D ] {O(d)} gates，就可以做到這件事了

170
00:08:11,371 --> 00:08:13,858
我想這個是大家都知道的事情

171
00:08:13,858 --> 00:08:18,094
但這就只是打個比方，而不是一個證明

172
00:08:18,094 --> 00:08:21,904
接下來，我們要講說

173
00:08:21,904 --> 00:08:24,211
deep 為什麼他有潛能

174
00:08:24,211 --> 00:08:28,683
有機會可以比 shallow 的 network 更好呢？

175
00:08:28,683 --> 00:08:31,854
在早年，所謂早年是指大概

176
00:08:31,854 --> 00:08:33,871
14年，15年 那個時候

177
00:08:33,871 --> 00:08:37,084
這個東西是 Ian Goodfellow 教科書裡面都有寫的

178
00:08:37,084 --> 00:08:39,056
所以他是比較早的東西，我們就知道說

179
00:08:39,056 --> 00:08:42,136
假設一個 network，它是 Shallow 跟 wide

180
00:08:42,136 --> 00:08:46,847
跟假設另外一個 network，他是 deep 而 narrow 的

181
00:08:46,847 --> 00:08:49,445
在他們有差不多參數量的情況下

182
00:08:49,445 --> 00:08:53,414
這個時候，shallow 而 wide 的 network

183
00:08:53,414 --> 00:08:57,082
他可以產生出來的 piecewise linear function 的這個

184
00:08:57,082 --> 00:08:59,663
piece 是比較少的

185
00:08:59,663 --> 00:09:01,494
而 deep, narrow 的架構

186
00:09:01,494 --> 00:09:05,653
他可以產生出來的 piece 是比較多的

187
00:09:05,653 --> 00:09:07,885
如果你今天有一個

188
00:09:10,991 --> 00:09:13,310
shallow 跟 deep 的 network，在相同參數量的情況下

189
00:09:13,310 --> 00:09:16,780
shallow 的 network 可以產生出來的 piece 是比較少的

190
00:09:16,780 --> 00:09:21,622
deep 的 network 可以產生出來的 piece 是比較多的

191
00:09:21,622 --> 00:09:22,666
怎麼說呢？

192
00:09:22,666 --> 00:09:27,449
我們先來看看，假設給你一個 network 的架構

193
00:09:27,449 --> 00:09:31,140
就隨便拿一個 ReLU 的 network 給你

194
00:09:31,140 --> 00:09:35,760
這個 network 會有多少的 piece 呢？

195
00:09:35,760 --> 00:09:39,330
他是 piecewise linear 的 function 嘛

196
00:09:39,330 --> 00:09:43,841
他會有多少個 linear 的片段呢？他會有多少的 piece 呢？

197
00:09:43,841 --> 00:09:46,450
我們先來看一下他的 upper bound

198
00:09:46,450 --> 00:09:48,491
就是最多會有多少

199
00:09:49,480 --> 00:09:50,917
怎麼想他的 upper bound 呢？

200
00:09:50,917 --> 00:09:53,523
我們大家都對於 ReLU network 都很熟嘛

201
00:09:53,523 --> 00:09:55,180
你知道在 ReLU network 裡面呢

202
00:09:55,180 --> 00:10:00,526
每一個 neuron 有兩個 operation 的 region，對不對？

203
00:10:00,526 --> 00:10:02,051
一個 operation 的 region 是

204
00:10:02,051 --> 00:10:05,297
這個 neuron 的 output 是 0

205
00:10:05,297 --> 00:10:06,929
另外一個 operation 的 region 是

206
00:10:06,929 --> 00:10:09,226
input [等於] {=} output

207
00:10:09,226 --> 00:10:12,717
那我們之前在 machine learning 的課裡面

208
00:10:12,717 --> 00:10:14,028
我們都講過 ReLU 的 network

209
00:10:14,028 --> 00:10:16,375
那時候會告訴你說，0 的那些

210
00:10:16,375 --> 00:10:18,860
就把它拿掉，他就好像不存在一樣

211
00:10:18,860 --> 00:10:23,368
剩下的部分，就好像是一個 linear 的 function

212
00:10:23,368 --> 00:10:24,590
這個時候很多人就會問我說

213
00:10:24,590 --> 00:10:27,864
其實 ReLU 只是 linear 的 function，那不是很弱嗎

214
00:10:27,864 --> 00:10:30,176
但他不完全是一個 linear 的 function，他是一個

215
00:10:30,176 --> 00:10:32,480
piecewise linear 的 function

216
00:10:32,480 --> 00:10:37,353
當今天你的 neuron 都作用在同樣的 region 的時候

217
00:10:37,353 --> 00:10:38,723
他是一個 linear function

218
00:10:38,723 --> 00:10:43,652
但是，他今天換了，有一些 neuron 的 region

219
00:10:43,652 --> 00:10:48,091
他的 operation 的狀態換了的時候

220
00:10:48,091 --> 00:10:51,623
他就進入了另一個 linear 的 region

221
00:10:51,623 --> 00:10:54,449
這樣大家應該知道我的意思吧，就是

222
00:10:54,449 --> 00:10:58,421
這個是你的 input, x，這個是你的 output, y

223
00:10:58,421 --> 00:11:07,982
在某種 activation function 的 mode 下面

224
00:11:07,982 --> 00:11:09,582
他是一個 linear 的 function

225
00:11:09,582 --> 00:11:12,311
但是你看今天 input 超過某個範圍

226
00:11:12,311 --> 00:11:15,412
某一個 neuron 的 operation 的 mode

227
00:11:15,412 --> 00:11:17,680
換掉了，就本來他可能是

228
00:11:17,680 --> 00:11:19,534
output 都是 [零] {0}，現在變成 input [等於] {=} output

229
00:11:19,534 --> 00:11:21,573
或他本來 input [等於] {=} output，現在變成 output 都是 0

230
00:11:21,573 --> 00:11:24,495
那你就變成另外一個 linear 的 function

231
00:11:24,495 --> 00:11:29,408
所以，今天我們來分析一個 ReLU 的 network

232
00:11:29,408 --> 00:11:32,576
他有幾個 piece，他的 upper bound 就是

233
00:11:32,576 --> 00:11:34,390
看你有幾個 neuron

234
00:11:34,390 --> 00:11:36,799
每一個 neuron 他都可以是

235
00:11:36,799 --> 00:11:40,065
作用在兩個 mode 的其中一個

236
00:11:40,065 --> 00:11:43,012
所以，所有的 neuron 的 mode 的可能性

237
00:11:43,012 --> 00:11:46,167
我們就這邊叫他 activation 的 pattern

238
00:11:46,167 --> 00:11:47,803
所以 activation pattern 的意思就是

239
00:11:47,803 --> 00:11:51,047
某一種 neuron 的 mode 的組合，比如說這邊是

240
00:11:51,047 --> 00:11:53,833
linear, linear, [零] {0}, [零] {0}

241
00:11:53,833 --> 00:11:56,371
[零] {0}, linear, [零] {0}, linear

242
00:11:56,371 --> 00:11:57,532
這個叫做一種 pattern

243
00:11:57,532 --> 00:11:59,182
你也可能有別種 pattern

244
00:12:00,269 --> 00:12:01,750
總共有多少種 pattern 呢？

245
00:12:01,750 --> 00:12:05,239
這個是國小數學，每一個 neuron 有兩種可能性所以

246
00:12:05,239 --> 00:12:07,978
假設有 n 個 neuron，就是有 [二的 n 次方] {2^n} 個

247
00:12:07,978 --> 00:12:09,691
可能的 activation pattern

248
00:12:09,691 --> 00:12:10,813
所以有 N 個 neuron，

249
00:12:10,813 --> 00:12:13,254
就有 [二的 n 次方] {2^n} 個可能的 activation pattern

250
00:12:13,254 --> 00:12:16,786
[八] {8} 個 neuron，就有 [二的八次方] {2^8} 個可能的 activation pattern

251
00:12:16,786 --> 00:12:21,858
每個 activation pattern 都只造了一個 linear function

252
00:12:21,858 --> 00:12:26,073
所以，想起來好像是有 N 個 neuron

253
00:12:26,073 --> 00:12:27,980
有 [二的 n 次方] {2^n} 個 activation pattern

254
00:12:27,980 --> 00:12:32,617
那我們用這一個 network 架構所訂出來的 function

255
00:12:32,617 --> 00:12:36,434
應該要有 [二的 n 次方] {2^n} 個 linear 的 pieces

256
00:12:36,434 --> 00:12:38,878
有 [二的 n 次方] {2^n} 個片段

257
00:12:38,878 --> 00:12:42,598
這個其實是一個 upper bound

258
00:12:42,598 --> 00:12:43,718
這個是一個 upper bound

259
00:12:43,718 --> 00:12:48,019
這個是一個最佳的狀況，你不可能

260
00:12:48,019 --> 00:12:51,630
事實上，為什麼這是一個最佳的狀況呢？因為

261
00:12:51,630 --> 00:12:54,446
為什麼這個狀況不一定能夠達到呢？

262
00:12:54,446 --> 00:12:55,532
因為有些 pattern

263
00:12:55,532 --> 00:12:57,444
可能是永遠沒有辦法出現的

264
00:12:57,444 --> 00:12:59,171
有些 pattern 是不可能出現的

265
00:12:59,171 --> 00:13:01,975
舉例來說，我們舉一個這樣子的例子

266
00:13:01,975 --> 00:13:04,956
這是一個非常簡單的 ReLU 的 network

267
00:13:04,956 --> 00:13:06,967
只有一層，只有兩個 neuron

268
00:13:06,967 --> 00:13:09,914
我們剛才說，每一個 neuron 有兩個 mode

269
00:13:09,914 --> 00:13:14,176
所以今天，按照這個 network，他 activation 的 pattern

270
00:13:14,176 --> 00:13:17,651
其實應該要有[四] {4}種，對不對，就是

271
00:13:17,651 --> 00:13:22,154
他的 activation 的 pattern，其實應該要有[四] {4}種

272
00:13:22,154 --> 00:13:24,832
但是，你實際上去想一想

273
00:13:24,832 --> 00:13:28,560
你會發現說，這一個 network 的架構

274
00:13:28,560 --> 00:13:33,378
他只定義得出，這三個 pieces 而已

275
00:13:33,378 --> 00:13:37,869
他定義不出[四] {4}個 pieces，對不對？因為

276
00:13:37,869 --> 00:13:42,217
有一些 operation 的狀態

277
00:13:42,217 --> 00:13:46,256
是不合理的，是沒有辦法呈現的

278
00:13:46,256 --> 00:13:49,199
我不知道大家能不能夠接受這個想法

279
00:13:49,199 --> 00:13:52,919
你可以回去想一下，你把這一個 network 的架構

280
00:13:52,919 --> 00:13:55,127
你把他的參數試不同的參數

281
00:13:55,149 --> 00:13:59,333
你挪來挪去，你可能就只造得出三個 piece 而已

282
00:13:59,333 --> 00:14:04,966
因為有一些，你只能產生三種 activation pattern

283
00:14:04,966 --> 00:14:08,574
你沒有辦法產生四種 activation pattern

284
00:14:08,574 --> 00:14:16,025
其實，事實上呢，今天假設我們有一個 network

285
00:14:16,025 --> 00:14:18,205
他是只有一個 hidden layer，有 n 個 neuron

286
00:14:18,205 --> 00:14:20,194
本來，按照 Upper Bound 來想

287
00:14:20,194 --> 00:14:25,043
應該要有 [二的 n 次方] {2^n} 個 activation pattern

288
00:14:25,043 --> 00:14:29,654
他應該可以產生 [二的 n 次方] {2^n} 個 piece

289
00:14:29,654 --> 00:14:32,688
但是，如果你仔細想一下的話，你會發現說

290
00:14:32,688 --> 00:14:35,365
今天只有一個 hidden layer 的 network

291
00:14:35,365 --> 00:14:37,706
他假設那個 hidden layer 的 neuron 是 n

292
00:14:37,706 --> 00:14:41,765
其實他弄得出來的 pice 的數目

293
00:14:41,765 --> 00:14:44,967
其實只是 [big O of n] {O(n)} 而已，可能是 [n 加一] {n+1} 這樣

294
00:14:44,967 --> 00:14:48,596
我們沒有辦法弄出 [二的 n 次方] {2^n} 個不同的 pieces

295
00:14:48,596 --> 00:14:53,922
這邊我們剛才講的是 upper bound

296
00:14:53,922 --> 00:14:55,634
所以從 Upper bound 看起來

297
00:14:55,634 --> 00:14:56,827
雖然我們知道說，那個 upper bound

298
00:14:56,827 --> 00:14:58,591
實際上可能是達不到的

299
00:14:58,591 --> 00:15:02,498
就是我們說，今天給我們一個 ReLU 的 network

300
00:15:02,498 --> 00:15:05,437
他最多可以製造出來的 function

301
00:15:05,437 --> 00:15:07,303
會有幾個 linear 的 piece 呢？

302
00:15:07,303 --> 00:15:09,797
最多是 [二的 n 次方] {2^n} 個 piece

303
00:15:09,797 --> 00:15:13,121
但這是一個 upper bound，很有可能，你怎麼調

304
00:15:13,121 --> 00:15:17,044
都達不到那一個 piece 的數目

305
00:15:17,044 --> 00:15:18,751
接下來，我們要講的是

306
00:15:18,751 --> 00:15:20,843
那 lower bound 呢？

307
00:15:20,843 --> 00:15:23,692
那你要講 lower bound 很簡單，就是

308
00:15:23,692 --> 00:15:27,946
兜一個 network，看看我們可以製造出多少個 piece

309
00:15:27,946 --> 00:15:30,469
那個 piece 就是我們可以製造出來的

310
00:15:30,469 --> 00:15:32,444
piece 的數目的 lower bound

311
00:15:32,444 --> 00:15:34,584
所以，我們就是講一個 network

312
00:15:34,584 --> 00:15:37,264
找一個 ReLU 的 network

313
00:15:37,264 --> 00:15:39,143
然後，分析一下說

314
00:15:39,143 --> 00:15:40,832
我們根據這個 ReLU 的 network

315
00:15:40,832 --> 00:15:43,554
我們可以製造出多少的 pieces

316
00:15:43,554 --> 00:15:48,498
那在講這一段之前呢

317
00:15:48,498 --> 00:15:51,941
我們先製造一個特別的 activation function

318
00:15:51,941 --> 00:15:53,986
這個特別的 activation function 叫做

319
00:15:53,986 --> 00:15:57,303
取絕對值的 activation function

320
00:15:57,303 --> 00:15:59,649
比如說，你的 input 是 x

321
00:15:59,649 --> 00:16:02,333
我們把 x 乘上 w，再加上 bias, b

322
00:16:02,333 --> 00:16:04,264
通過這個 activation function 以後呢

323
00:16:04,264 --> 00:16:06,127
會取絕對值

324
00:16:06,127 --> 00:16:10,739
怎麼 implement 這種取絕對值的 activation function 呢

325
00:16:10,739 --> 00:16:13,605
其實你可以把兩個 ReLU 的 activation function

326
00:16:13,605 --> 00:16:15,893
組合起來，就可以變成一個

327
00:16:15,893 --> 00:16:18,838
取絕對值的 activation function

328
00:16:18,838 --> 00:16:22,835
怎麼做？我們今天有兩個 ReLU 的 neuron

329
00:16:22,835 --> 00:16:24,638
我們有兩個 ReLU 的 neuron

330
00:16:24,638 --> 00:16:25,817
今天 x 進來

331
00:16:25,817 --> 00:16:27,894
走上面這個 ReLU 的 neuron 的時候

332
00:16:27,894 --> 00:16:29,639
他乘以 w，再加上 bias

333
00:16:29,639 --> 00:16:31,742
走下面這個 ReLU 的 neuron 的時候

334
00:16:31,742 --> 00:16:35,094
他乘上 [負 w] {-w}，再減掉 bias, b

335
00:16:35,094 --> 00:16:38,236
今天，這兩個都是

336
00:16:38,236 --> 00:16:42,231
這兩個都是 ReLU 的 activation function

337
00:16:42,231 --> 00:16:47,441
所以，如果今天，[w x 加 b 大於 零] {(w*x + b) > 0} 的話

338
00:16:47,441 --> 00:16:51,135
那就是拿這邊的輸出，這邊的輸出就是 [零] {0} 嘛

339
00:16:51,135 --> 00:16:54,008
如果 [w x 加 b 小於 零] {(w*x + b) < 0} 的話

340
00:16:54,008 --> 00:16:57,682
就是拿這邊的輸出，這邊就是 [零] {0} 嘛

341
00:16:57,682 --> 00:16:59,490
所以，仔細想一下就會知道說

342
00:16:59,490 --> 00:17:03,125
用這個方法，你用 ReLU 的 activation function

343
00:17:03,125 --> 00:17:07,823
你可以製造一個取絕對值的 activation function

344
00:17:07,823 --> 00:17:11,355
你可以去製造一個

345
00:17:11,355 --> 00:17:14,301
像這樣子取絕對值的 activation function

346
00:17:14,301 --> 00:17:19,854
那接下來呢

347
00:17:19,854 --> 00:17:21,681
我們要講的事情是

348
00:17:21,681 --> 00:17:26,510
如果我們現在有這樣子的 activation function 的話

349
00:17:26,510 --> 00:17:29,908
像這樣子的 activation function

350
00:17:29,908 --> 00:17:32,069
我們就放一個這樣子的 neuron

351
00:17:32,069 --> 00:17:34,524
然後，我們的 input 是 x

352
00:17:34,524 --> 00:17:36,665
我們的 output 是 [a one] {a1}

353
00:17:36,665 --> 00:17:41,367
我們 input 的 x 跟 output 的 [a one] {a1} 間

354
00:17:41,367 --> 00:17:43,942
可能會有什麼樣的關係呢？

355
00:17:43,942 --> 00:17:45,892
他們的關係可能是

356
00:17:45,892 --> 00:17:48,740
這個樣子，可能是這個樣子

357
00:17:48,740 --> 00:17:52,765
在 [零] {0} 到 [二分之一] {1/2} 中間

358
00:17:52,765 --> 00:17:58,197
他的值是從[一] {1}逐漸下降到 [零] {0}

359
00:17:58,197 --> 00:18:00,806
從 [二分之一] {1/2} 到[一] {1}中間

360
00:18:00,806 --> 00:18:06,378
今天 output [a one] {a1} 的值是從 [二分之一] {1/2} 逐漸上升到 [一] {1}

361
00:18:06,378 --> 00:18:08,119
那我們假設說

362
00:18:08,119 --> 00:18:11,722
第一個 hidden layer 做的事情就是這樣

363
00:18:11,722 --> 00:18:15,438
那我現在假設我們加上第二個 hidden layer

364
00:18:15,438 --> 00:18:18,108
第二個 hidden layer 就是把 [a one] {a1} 吃進去

365
00:18:18,108 --> 00:18:21,082
然後，變成 [a two] {a2} 吐出來

366
00:18:21,082 --> 00:18:22,466
我們假設

367
00:18:22,466 --> 00:18:26,132
第二個 hidden layer 跟第一個 hidden layer 做的事情

368
00:18:26,132 --> 00:18:28,290
其實是一樣的

369
00:18:28,290 --> 00:18:30,156
[a one] {a1} 跟 [a two] {a2} 的關係

370
00:18:30,156 --> 00:18:33,184
和 x 和 [a one] {a1} 的關係，其實是一樣的

371
00:18:33,184 --> 00:18:36,406
當 [a one] {a1} 的變化從[零] {0}到 [二分之一] {1/2} 的時候

372
00:18:36,406 --> 00:18:38,084
[a two] {a2} 從[一] {1}下降到 [零] {0}

373
00:18:38,084 --> 00:18:41,282
當 [a one] {a1} 從 [二分之一] {1/2} 變到[一] {1}的時候

374
00:18:41,282 --> 00:18:43,134
[a two] {a2} 從[零] {0}上升到 [一] {1}

375
00:18:43,134 --> 00:18:47,152
x one 和 [a one] {a1} 的關係和 [a one] {a1} 和 [a two] {a2} 的關係，其實是一樣的

376
00:18:47,152 --> 00:18:49,800
那我們知道 x 和 [a one] {a1} 的關係

377
00:18:49,800 --> 00:18:51,514
我們就知道 [a one] {a1} 和 [a two] {a2} 的關係

378
00:18:51,514 --> 00:18:54,184
那這整個 function 長什麼樣子呢？

379
00:18:54,184 --> 00:18:57,223
也就是說，x 跟 [a two] {a2} 之間的關係

380
00:18:57,223 --> 00:18:59,479
像是什麼樣子呢？

381
00:18:59,479 --> 00:19:01,575
我們來想一想

382
00:19:02,742 --> 00:19:05,245
input 是從[零] {0}到 [一] {1}

383
00:19:05,245 --> 00:19:07,710
我們現在就是要看說，x 從[零] {0}到[一] {1}的時候

384
00:19:07,710 --> 00:19:11,377
[a two] {a2} 是怎麼樣變化

385
00:19:11,377 --> 00:19:12,562
是怎麼樣變化

386
00:19:12,562 --> 00:19:15,011
那今天這個 [a two] {a2} 呢

387
00:19:15,011 --> 00:19:18,912
在 [a one] {a1} 是 [二分之一] {1/2} 的地方

388
00:19:18,912 --> 00:19:20,851
會有一個轉折的點

389
00:19:20,851 --> 00:19:24,095
所以我們在考慮的時候，畫一個 [a one] {a1} [等於] {=} [二分之一] {1/2} 的線

390
00:19:24,095 --> 00:19:27,110
然後，從[零] {0}到[一] {1}之間

391
00:19:27,110 --> 00:19:29,818
我們分成四個區段來考慮

392
00:19:29,818 --> 00:19:32,420
我們考慮[零] {0}到 [四分之一] {1/4}

393
00:19:32,420 --> 00:19:34,296
考慮[零] {0}到 [四分之一] {1/4}

394
00:19:34,296 --> 00:19:36,329
[四分之一] {1/4} 到 [二分之一] {1/2}

395
00:19:36,329 --> 00:19:37,954
[二分之一] {1/2} 到 [四分之三] {3/4}

396
00:19:37,954 --> 00:19:41,705
[四分之三] {3/4} 到[一] {1}，分成四個片段來考慮

397
00:19:41,705 --> 00:19:44,160
如果考慮第一個片段

398
00:19:44,160 --> 00:19:48,831
考慮第一個片段，x 從[零] {0}變到 [二分之一] {1/2} 的時候

399
00:19:48,831 --> 00:19:52,692
x 從[零] {0}變到 [二分之一] {1/2} 的時候

400
00:19:52,692 --> 00:19:58,000
[a one] {a1} 從[一] {1}變到 [二分之一] {1/2}

401
00:19:58,000 --> 00:20:01,066
[a one] {a1} 從[一] {1}變到 [二分之一] {1/2}

402
00:20:01,066 --> 00:20:03,908
那 [a one] {a1} 從[一] {1}變到 [二分之一] {1/2} 的時候，[a two] {a2} 呢？

403
00:20:03,908 --> 00:20:05,285
他是從[一] {1}變到 0

404
00:20:05,285 --> 00:20:08,503
所以今天 x 從[零] {0}到 [四分之一] {1/4} 這段距離

405
00:20:08,503 --> 00:20:12,039
當從[零] {0}到 [四分之一] {1/4} 這段距離有變化的時候呢

406
00:20:12,039 --> 00:20:14,600
[a two] {a2} 是從[一] {1}變到[零] {0}的

407
00:20:14,600 --> 00:20:16,599
這個是第一段

408
00:20:16,599 --> 00:20:20,921
那如果你分析第二段從 [四分之一] {1/4} 到 [二分之一] {1/2} 的時候

409
00:20:20,921 --> 00:20:22,901
分析從 [四分之一] {1/4} 到 [二分之一] {1/2} 的時候

410
00:20:22,901 --> 00:20:25,766
x 的變化從 [四分之一] {1/4} 到 [二分之一] {1/2}

411
00:20:25,766 --> 00:20:30,703
[a one] {a1} 的變化是從 [二分之一] {1/2} 一直跑到 [零] {0}

412
00:20:30,703 --> 00:20:32,933
它從 [二分之一] {1/2} 一直跑到 [零] {0}

413
00:20:32,933 --> 00:20:36,266
這個其實非常難想像啦

414
00:20:36,266 --> 00:20:38,678
我不知道大家聽不聽得懂這樣子

415
00:20:38,678 --> 00:20:40,490
如果你聽不懂的話，你就自己回去

416
00:20:40,490 --> 00:20:44,106
你就自己仔細想一想，這個畫圖也不知道要怎麼畫才好

417
00:20:44,106 --> 00:20:47,463
這段 x 是從 [四分之一] {1/4} 到 [二分之一] {1/2}

418
00:20:47,463 --> 00:20:50,419
那 [a one] {a1} 是從 [二分之一] {1/2} 變到 [零] {0}

419
00:20:50,419 --> 00:20:52,242
所以 [a one] {a1} 是從 [二分之一] {1/2} 變到 [零] {0}

420
00:20:52,242 --> 00:20:55,022
[a two] {a2} 是從[零] {0}變到 1

421
00:20:55,022 --> 00:20:57,973
所以， x 從這邊到中間

422
00:20:57,973 --> 00:20:59,561
從 [四分之一] {1/4} 到 [二分之一] {1/2} 的時候

423
00:20:59,561 --> 00:21:01,891
[a two] {a2} 是從[零] {0}變到[一] {1}的

424
00:21:01,891 --> 00:21:03,603
所以是這個樣子

425
00:21:03,603 --> 00:21:07,043
再講下去你可能就會覺得有點無聊了

426
00:21:07,043 --> 00:21:09,871
所以，今天 x 從 [二分之一] {1/2} 變到 [四分之一] {1/4}

427
00:21:09,871 --> 00:21:13,459
那 [a two] {a2} 會有什麼樣的變化呢？他的變化是怎麼樣

428
00:21:13,459 --> 00:21:17,566
x 從 [四分之三] {3/4} 變到 1

429
00:21:17,566 --> 00:21:20,679
[a two] {a2} 的變化是這個樣子，所以他畫了一個

430
00:21:20,679 --> 00:21:25,308
w 的形狀，他畫了一個 w 的形狀

431
00:21:25,308 --> 00:21:27,595
所以，今天我們知道說

432
00:21:27,595 --> 00:21:31,151
[a two] {a2} 的輸出是這個 function

433
00:21:31,151 --> 00:21:36,432
有兩個 neuron 的 function，他是 w 的形狀

434
00:21:36,432 --> 00:21:39,669
那我們再加上第三個 neuron 呢？

435
00:21:39,669 --> 00:21:41,838
如果再加上第三個 neuron 的話

436
00:21:41,838 --> 00:21:43,573
會發生什麼樣的事情呢？

437
00:21:43,573 --> 00:21:44,875
我們假設第三個 neuron

438
00:21:44,875 --> 00:21:48,038
跟前面兩個 neuron 做的事情是一模一樣的

439
00:21:48,038 --> 00:21:49,472
只是 input 是 [a two] {a2}

440
00:21:49,472 --> 00:21:52,261
output 是 [a 三] {a3}，那 [a two] {a2} 跟 [a 三] {a3} 的關係

441
00:21:52,261 --> 00:21:54,127
長得是這個樣子

442
00:21:54,127 --> 00:21:57,164
當我們加上這個紅色的線的時候呢

443
00:21:57,164 --> 00:21:58,910
你就會發現說

444
00:21:58,910 --> 00:22:01,431
你加這個紅色線以後

445
00:22:01,431 --> 00:22:05,202
接下來，你就分析一下這個 x 從[零] {0}變到 [一] {1}

446
00:22:05,202 --> 00:22:09,144
你要分成[八] {8}個區間去考慮

447
00:22:09,144 --> 00:22:11,175
[八] {8} 個區間去考慮

448
00:22:11,175 --> 00:22:14,282
你要從[零] {0}分析到 [八分之一] {1/8}

449
00:22:14,282 --> 00:22:18,669
就 x 從[零] {0}變到 [八分之一] {1/8} 的時候

450
00:22:18,669 --> 00:22:22,090
[a two] {a2} 是從[一] {1}跑到 [二分之一] {1/2}

451
00:22:22,090 --> 00:22:24,416
是從[一] {1}跑到 [二分之一] {1/2}

452
00:22:24,416 --> 00:22:24,713
所以 [a 三] {a3} 會從[一] {1}變到 [零] {0}

453
00:22:27,824 --> 00:22:29,650
所以 [a 三] {a3} 會從[一] {1}變到 [零] {0}

454
00:22:29,650 --> 00:22:31,749
然後你就把每一個

455
00:22:33,721 --> 00:22:35,662
piece，每一個小段啊

456
00:22:35,662 --> 00:22:40,051
這個 x 跟 [a two] {a2}，[a two] {a2} 到 [a 三] {a3} 的關係，通通畫出來

457
00:22:40,051 --> 00:22:42,292
看起來就是這個樣子，所以

458
00:22:42,292 --> 00:22:44,344
本來是一個 w 的形狀

459
00:22:44,344 --> 00:22:47,648
現在變成兩個 ｗ 的形狀

460
00:22:47,648 --> 00:22:52,430
變成兩個 ｗ 連在一起，變成很多鋸齒的形狀

461
00:22:52,430 --> 00:22:53,963
所以，現在你就會發現說

462
00:22:53,963 --> 00:22:57,391
本來 [a one] {a1} 他是長這個樣子

463
00:22:57,391 --> 00:23:00,115
就 [a two] {a2} 跟 [a 三] {a3} 的關係和 x 跟 [a one] {a1} 的關係長得是一樣

464
00:23:00,115 --> 00:23:00,773
他們長這樣

465
00:23:00,773 --> 00:23:05,410
他總共有兩個線段，就 [二的一次方] {2^1} 個線段

466
00:23:05,410 --> 00:23:07,986
到 [a two] {a2} 的時候，他總共有

467
00:23:07,986 --> 00:23:10,804
4 個線段，[二的二次方] {2^2} 個線段

468
00:23:10,804 --> 00:23:13,812
到 [a 三] {a3} 的時候，它就變成有

469
00:23:13,812 --> 00:23:16,570
[二的三次方] {2^2} 個線段

470
00:23:16,570 --> 00:23:18,821
所以，你會發現說呢

471
00:23:18,821 --> 00:23:23,080
你會發現說呢，今天當我們用 deep structure 的時候

472
00:23:23,080 --> 00:23:25,133
每次我多加了一個 layer

473
00:23:25,133 --> 00:23:27,742
其實我的每個 layer 只有兩個 neuron

474
00:23:27,742 --> 00:23:30,164
我每次多加了兩個 neuron 的時候

475
00:23:30,164 --> 00:23:33,460
我每次多加了兩個 neuron 的時候

476
00:23:33,460 --> 00:23:36,062
我們可以產生的 linear 的 region

477
00:23:36,062 --> 00:23:39,800
可以產生出來的線段的數目

478
00:23:39,800 --> 00:23:43,307
就會變成兩倍，就每多加兩個

479
00:23:43,307 --> 00:23:46,486
你的線段的數目就會 double

480
00:23:46,486 --> 00:23:48,353
所以，如果我們今天比較

481
00:23:48,353 --> 00:23:50,506
shallow 的 network 跟 deep 的 network

482
00:23:50,506 --> 00:23:52,244
在講 shallow 的 network 的時候

483
00:23:52,244 --> 00:23:55,390
我們每次兩個 neuron 組合起來才產生一個線段

484
00:23:55,390 --> 00:23:58,010
所以今天如果你要產生 [一百] {100} 個線段

485
00:23:58,010 --> 00:23:59,266
你就要 [兩百] {200} 個 neuron

486
00:23:59,266 --> 00:24:02,438
但是，今天如果是 deep 的 structure

487
00:24:02,438 --> 00:24:05,840
每次多加了兩個 neuron ，這邊這個

488
00:24:05,840 --> 00:24:08,143
取絕對值的這個 neuron

489
00:24:08,143 --> 00:24:09,871
就是他是兩個 ReLU 所組成的

490
00:24:09,871 --> 00:24:11,336
每次多加一個 layer

491
00:24:11,336 --> 00:24:13,122
那個 layer 就只有兩個 neuron

492
00:24:13,122 --> 00:24:16,840
你都可以讓你的線段的數目多增加兩倍

493
00:24:16,840 --> 00:24:18,083
所以，你今天

494
00:24:18,083 --> 00:24:22,337
舉例來說，你要產生 [一百] {100} 個線段

495
00:24:22,337 --> 00:24:25,568
[一百] {100} 個線段是多少？

496
00:24:25,568 --> 00:24:31,321
我只要 [二] {2} 的 [七] {7} 次方

497
00:24:31,321 --> 00:24:33,141
對不對，我只要 [七] {7} 個 neuron

498
00:24:33,141 --> 00:24:35,345
我就可以做到那件事情了

499
00:24:35,345 --> 00:24:38,249
結果發現，你要產生有比較多片段的

500
00:24:38,249 --> 00:24:41,738
你要產生有比較多片段的

501
00:24:41,738 --> 00:24:46,955
function 的時候，用 deep 是比較有效率的

502
00:24:46,955 --> 00:24:48,870
如果你仔細回想一下

503
00:24:48,870 --> 00:24:53,639
為什麼 deep 可以產生比較多的線段

504
00:24:53,639 --> 00:24:58,004
他做的事情比較像是摺紙一樣

505
00:24:58,004 --> 00:25:01,399
對不對？或者是他其實是把同樣的 pattern

506
00:25:01,399 --> 00:25:04,251
反覆的出現，就本來你只有一個 v

507
00:25:04,251 --> 00:25:06,611
然後第一層這個 v

508
00:25:06,611 --> 00:25:09,196
第二層再把兩個 v 接起來變成 w

509
00:25:09,196 --> 00:25:11,195
第三層則是把兩個 w 接起來

510
00:25:11,195 --> 00:25:14,511
變成有兩個 ｗ 拼在一起

511
00:25:14,511 --> 00:25:17,563
接下來，下一層就把兩個 w

512
00:25:17,563 --> 00:25:19,887
再 double，變成有[四] {4}個 w

513
00:25:19,887 --> 00:25:24,396
他是這樣的，他是把原來你的 piece 不斷的反覆產生

514
00:25:24,396 --> 00:25:27,719
有點像這個，不知道大家能不能體會，他就像是

515
00:25:27,719 --> 00:25:31,180
那個雪花結晶的結構這樣

516
00:25:31,180 --> 00:25:34,675
他不斷地把原來的 pattern，不斷地複製

517
00:25:34,675 --> 00:25:38,315
他可以產生很多個 piece，但那些 piece 是有規則的

518
00:25:38,315 --> 00:25:42,912
那講了這麼多以後呢

519
00:25:42,912 --> 00:25:45,491
其實你可以非常輕易地

520
00:25:45,491 --> 00:25:49,874
就證明說，假設你現在的 network

521
00:25:49,874 --> 00:25:53,035
寬度是 k，深度是 h

522
00:25:53,035 --> 00:25:57,980
那你可以製造 [k的h次方] {k^h} 個片段

523
00:25:57,980 --> 00:25:59,868
我們剛才是寬度是 [二] {2}

524
00:25:59,868 --> 00:26:03,386
我們可以製造出 [二的h次方] {2^h} 個

525
00:26:03,386 --> 00:26:07,239
就寬度是 [二] {2}，深度是 h

526
00:26:07,239 --> 00:26:09,962
我們可以製造 [二的h次方] {2^h} 個片段

527
00:26:09,962 --> 00:26:13,975
那現在你可以輕易地自己想出來說，假設現在

528
00:26:13,975 --> 00:26:15,798
寬度不是 2 而是 k

529
00:26:15,798 --> 00:26:19,480
那深度是 h，你可以製造出

530
00:26:19,480 --> 00:26:21,756
[k的h次方] {k^h} 個片段

531
00:26:21,756 --> 00:26:24,366
其實你可以非常輕易地想出

532
00:26:24,366 --> 00:26:27,291
找到一個 network 可以做一件事情

533
00:26:27,291 --> 00:26:29,581
那這個東西就是這個 network 的架構

534
00:26:29,581 --> 00:26:32,293
可以產生的 piece 的數目的 lower bound

535
00:26:32,293 --> 00:26:35,914
所以，今天這個結果告訴我們什麼

536
00:26:35,914 --> 00:26:39,588
告訴我們說，你可以產生的片段的數目啊

537
00:26:39,588 --> 00:26:41,626
是 [k的h次方] {k^h} 次方

538
00:26:41,626 --> 00:26:45,552
所以，h 也就是深度，是放在指數的地方

539
00:26:45,552 --> 00:26:47,918
所以，當你增加你的深度的時候

540
00:26:47,918 --> 00:26:51,677
你可以非常快的增加 piece 的數目

541
00:26:51,677 --> 00:26:54,439
如果你想要知道更多更細的證明的話

542
00:26:54,439 --> 00:26:56,685
下面列了一大堆 paper 這樣

543
00:26:56,685 --> 00:26:58,514
舉例來說，第一篇這個

544
00:26:58,514 --> 00:27:00,350
前面兩篇是這個 Bengio 的 paper

545
00:27:00,350 --> 00:27:02,386
其實是最早做這樣子分析的 paper

546
00:27:02,386 --> 00:27:04,317
那在裡面呢，因為我們今天

547
00:27:04,317 --> 00:27:07,073
在上課的時候，我們都假設 input 只有一個 dimension

548
00:27:07,073 --> 00:27:09,787
那他們不是這樣，他們會假設比較複雜的 case

549
00:27:09,787 --> 00:27:13,803
就 input 是第一個 dimension

550
00:27:13,803 --> 00:27:15,641
那這個時候狀況就比較複雜了

551
00:27:15,641 --> 00:27:18,228
沒有今天得到的式子那麼單純

552
00:27:18,228 --> 00:27:20,791
原來在 ICLR, 2014 的 paper 裡面，他們

553
00:27:20,791 --> 00:27:21,945
得到了一個 lower bound

554
00:27:21,945 --> 00:27:23,851
後來呢，他們又在

555
00:27:23,851 --> 00:27:25,516
下一篇 NIPS, 2014 裡面

556
00:27:25,516 --> 00:27:27,380
就 improve 了那一個 lower bound

557
00:27:27,380 --> 00:27:30,634
後來也有很多人做類似的嘗試去做

558
00:27:30,634 --> 00:27:32,871
去繼續 improve 那個 lower bound

559
00:27:32,871 --> 00:27:36,348
那就把這個文獻列在這邊給大家參考

560
00:27:36,348 --> 00:27:38,387
那我想要講的是最後一篇

561
00:27:38,387 --> 00:27:40,816
在最後一篇除了有理論的證明之外

562
00:27:40,816 --> 00:27:42,877
他還做了一些實驗

563
00:27:42,877 --> 00:27:44,374
因為我們剛才講的只是一個

564
00:27:44,374 --> 00:27:46,325
理論上是這個樣子

565
00:27:46,325 --> 00:27:48,764
就我們用了一個很奇怪的方法

566
00:27:48,764 --> 00:27:50,782
兜出了一個 ReLU 的 network

567
00:27:50,782 --> 00:27:53,077
然後分析說，嗯，這個 ReLU 的 network

568
00:27:53,077 --> 00:27:55,051
他有很多很多的 piece

569
00:27:55,051 --> 00:27:56,665
可是你可能就覺得說，欸

570
00:27:56,665 --> 00:27:59,628
這個會不會是一個非常非常 specefic 的 case

571
00:27:59,628 --> 00:28:02,222
也許在一個正常的狀況下，你 train 一個 network

572
00:28:02,222 --> 00:28:04,996
你根本就不會產生那麼多的 piece

573
00:28:04,996 --> 00:28:07,161
所以，他就做了一個

574
00:28:07,161 --> 00:28:10,067
做了一些實驗來 verify 這些事

575
00:28:10,067 --> 00:28:13,023
他有一個實驗是做在 MNIST 上

576
00:28:13,023 --> 00:28:14,223
在 MNIST 上面

577
00:28:14,223 --> 00:28:16,824
你這個有不同的

578
00:28:16,824 --> 00:28:18,176
我們先看第一個圖

579
00:28:18,176 --> 00:28:20,135
第一個圖的橫坐標代表的是

580
00:28:20,135 --> 00:28:21,646
network 的深度

581
00:28:21,646 --> 00:28:25,372
[兩] {2} 層,[四] {4}層, [六] {6} 層,[八] {8}層, [十] {10} 層, [十二] {12} 層等等

582
00:28:25,372 --> 00:28:27,980
那不同的顏色代表不同的寬度

583
00:28:27,980 --> 00:28:29,540
[五十] {50} 個 neuron, [一百] {100}  個 neuron

584
00:28:29,540 --> 00:28:31,377
[五百] {500} 個 neuron, [七百] {700} 個 neuron 等等

585
00:28:31,377 --> 00:28:33,613
然後這個 scl 啊，大家不用太在意他

586
00:28:33,613 --> 00:28:38,028
他是 train network 的時候不同的 initialization 的參數

587
00:28:38,028 --> 00:28:40,116
接下來這邊他做的實驗就是

588
00:28:40,116 --> 00:28:42,834
他把這個 network 先拿出來

589
00:28:42,834 --> 00:28:44,863
然後再看說， input 從

590
00:28:44,863 --> 00:28:46,676
某一點到某一點的時候

591
00:28:46,676 --> 00:28:49,781
output 總共通過了多少個 piece

592
00:28:49,781 --> 00:28:51,957
了解嗎？network 就是一個 function 啊

593
00:28:51,957 --> 00:28:54,400
就是一個 function ，piecewise linear function

594
00:28:54,400 --> 00:28:56,650
然後他就算說，從某一點

595
00:28:56,650 --> 00:29:00,702
到某一點總共經過了多少個 pieces

596
00:29:00,702 --> 00:29:02,172
那今天這個圖啊

597
00:29:02,172 --> 00:29:04,545
縱軸就是 pieces 的數目

598
00:29:04,545 --> 00:29:05,991
那注意一下這個縱軸

599
00:29:05,991 --> 00:29:09,473
他是 exponential 的

600
00:29:10,679 --> 00:29:12,784
所以今天這個直線的上升

601
00:29:12,784 --> 00:29:15,038
其實是 exponential 的上升

602
00:29:15,038 --> 00:29:17,322
所以今天固定你的 network 的 size

603
00:29:17,322 --> 00:29:19,349
固定你的 network 的寬度，不是 size

604
00:29:19,349 --> 00:29:20,942
固定 network 的寬度

605
00:29:20,942 --> 00:29:23,153
增加深度的時候

606
00:29:23,153 --> 00:29:26,549
你會發現，這個時候你產生 pieces 的量

607
00:29:26,549 --> 00:29:28,172
就算在實際的 case

608
00:29:28,172 --> 00:29:29,782
他也是 exponential 增加的

609
00:29:29,782 --> 00:29:32,170
剛才講說，那個 upper bound 就是

610
00:29:32,170 --> 00:29:34,955
寬 k 的深度的 h 次方

611
00:29:34,955 --> 00:29:38,563
那這是一個理論上的

612
00:29:38,563 --> 00:29:40,433
想像，但是在實際上

613
00:29:40,433 --> 00:29:42,586
在實際的 application 上

614
00:29:42,586 --> 00:29:44,306
你可以觀察到這樣的現象

615
00:29:44,334 --> 00:29:46,249
當你的 depth 增加的時候

616
00:29:46,249 --> 00:29:49,328
你產生的 piece 的數目是 exponential 增加

617
00:29:49,328 --> 00:29:50,931
另外，這個實驗是

618
00:29:50,931 --> 00:29:56,043
layer 的數目固定，但是橫軸是改變 layer 的寬度

619
00:29:56,043 --> 00:29:57,746
有 [兩百] {200} 個 neuron, [四百] {400} 個 neuron

620
00:29:57,746 --> 00:29:59,500
[六百] {600} 個 neuron, [八百] {800} 個 neuron 等等

621
00:29:59,500 --> 00:30:01,510
縱軸，一樣是 exponential

622
00:30:01,510 --> 00:30:04,043
不同的顏色代表不同的深度

623
00:30:04,043 --> 00:30:06,067
兩層、四層、六層等等

624
00:30:06,067 --> 00:30:09,007
你會發現說，今天如果是固定你的深度

625
00:30:09,007 --> 00:30:11,342
但是，只改變你的寬度的時候

626
00:30:11,342 --> 00:30:15,012
對產生的 piece 的數目會影響有多大？這個線呢

627
00:30:15,012 --> 00:30:17,612
基本上看起來像是直線一樣

628
00:30:17,612 --> 00:30:19,709
這個是第一個實驗

629
00:30:19,709 --> 00:30:22,374
他做了另外一個有趣的實驗是說

630
00:30:22,374 --> 00:30:23,831
他假如他這樣做

631
00:30:23,831 --> 00:30:25,724
就拿出一個 network

632
00:30:25,724 --> 00:30:27,949
他 paper 裡面其實沒有講得很清楚

633
00:30:27,949 --> 00:30:29,329
這個 network 是哪來的

634
00:30:29,329 --> 00:30:31,668
拿出一個 network 有很多層

635
00:30:31,668 --> 00:30:34,482
那他在 input 的這個 space 上面啊

636
00:30:34,482 --> 00:30:38,092
畫一個圈圈，假設 input 是二維的，畫一個圈圈

637
00:30:38,092 --> 00:30:40,903
那通過第一個 hidden layer 以後

638
00:30:40,903 --> 00:30:42,986
那些 neuron 不是會有 output 嗎？

639
00:30:42,986 --> 00:30:45,796
不是會變成一個 [一百] {100} 維的 vector 嗎？input 二維

640
00:30:45,796 --> 00:30:47,912
假設你的第一個 hidden layer 是 [一百] {100} 維

641
00:30:47,912 --> 00:30:49,670
他會變成一個 [一百] {100} 維的點嘛

642
00:30:49,670 --> 00:30:52,620
但是你在 input 的時候，你是畫一個圈圈

643
00:30:52,620 --> 00:30:55,418
那在高維的空間中

644
00:30:55,418 --> 00:30:58,696
你也是走了某一個軌跡，對不對？

645
00:30:58,696 --> 00:31:02,203
假設你的 hidden layer size 是 [一百] {100} 維

646
00:31:02,203 --> 00:31:04,472
那在 [一百] {100} 維的空間中，你也是走了某一個軌跡

647
00:31:04,472 --> 00:31:05,788
只是它不見得是圓的

648
00:31:05,788 --> 00:31:07,603
他就把這個 [一百] {100} 維的軌跡

649
00:31:07,603 --> 00:31:09,890
[一百] {100} 維空間中的軌跡 project 到二維，他說

650
00:31:09,890 --> 00:31:12,258
看起來像是這個樣子，這是第一個 layer

651
00:31:12,258 --> 00:31:14,517
第二個 layer 看起來像是這個樣子

652
00:31:14,517 --> 00:31:16,191
第三個 layer 看起來像是這個樣子

653
00:31:16,191 --> 00:31:17,664
第四個 layer 看起來就是這樣

654
00:31:17,664 --> 00:31:20,880
就越來越複雜，就本來你在 input 的時候

655
00:31:20,880 --> 00:31:23,240
你只是畫了一個圈圈

656
00:31:23,240 --> 00:31:26,011
但是，通過很多個 layer 以後

657
00:31:26,011 --> 00:31:29,307
這個軌跡，在 nerwork 的 output 看起來

658
00:31:29,307 --> 00:31:30,493
越來越複雜

659
00:31:30,493 --> 00:31:33,253
這個軌跡越來越複雜，直到最後變得非常的複雜

660
00:31:33,253 --> 00:31:35,490
你會發現說，這個複雜的結構裡面

661
00:31:35,490 --> 00:31:37,844
其實，如果你仔細看一下，他是有 pattern 的

662
00:31:37,844 --> 00:31:40,769
他並不是一個完全隨機的複雜的結構

663
00:31:40,769 --> 00:31:42,928
而是有某一些的對稱性這樣

664
00:31:42,928 --> 00:31:46,108
這邊有一些，某一些有趣的對稱性

665
00:31:46,108 --> 00:31:48,437
就好像是，雪花那個樣子

666
00:31:48,437 --> 00:31:50,613
所以，就呼應我們剛才講的

667
00:31:50,613 --> 00:31:52,678
說 network 他可以產生很多的 piece

668
00:31:52,678 --> 00:31:56,473
那產生的這些 piece 中間，他是有某一種 pattern 的

669
00:31:56,473 --> 00:32:00,295
他是把 v 變成 w，再把兩個 w 接起來這樣

670
00:32:00,295 --> 00:32:02,853
他是有一個固定的 pattern，他不是隨機的

671
00:32:02,853 --> 00:32:04,616
產生那些 piece

672
00:32:04,616 --> 00:32:08,095
那個 paper 還有另外一個實驗，

673
00:32:08,095 --> 00:32:09,848
這個實驗想要驗證的是說

674
00:32:09,848 --> 00:32:11,836
low layer 的參數

675
00:32:11,836 --> 00:32:14,124
就比較靠近 input 的那個 layer 的參數

676
00:32:14,124 --> 00:32:16,899
相較於比較靠近 output layer 的參數

677
00:32:16,899 --> 00:32:20,781
他是比較重要的，所以比較靠近 input 的那些參數

678
00:32:20,781 --> 00:32:22,557
是比較重要的，因為在直覺上

679
00:32:22,557 --> 00:32:24,430
想起來顯然是有道理的，因為

680
00:32:24,430 --> 00:32:27,003
我們今天在做 deep learning 的時候

681
00:32:27,003 --> 00:32:28,669
就好像是在折紙一樣

682
00:32:28,669 --> 00:32:31,778
前面的 layer 做的事情就是去摺紙

683
00:32:31,778 --> 00:32:34,996
那在折紙的時候，第一次對折是最重要的

684
00:32:34,996 --> 00:32:36,756
對不對，你第一次對折就歪掉了

685
00:32:36,756 --> 00:32:38,562
下面你再折就通通是歪的

686
00:32:38,562 --> 00:32:41,014
所以，這一個實驗想要驗證的是

687
00:32:41,014 --> 00:32:42,914
low layer 的參數是比較重要的

688
00:32:42,914 --> 00:32:46,230
那怎麼驗證呢？他先做了一下 [CIFAR ten] {CIFAR-10}

689
00:32:46,230 --> 00:32:51,428
他在 [CIFAR ten] {CIFAR-10} 上面拿出一個正確率非常高的 network

690
00:32:51,428 --> 00:32:54,036
然後，在 network 的參數上面

691
00:32:54,036 --> 00:32:56,121
加一些 noise，那分別加在

692
00:32:56,121 --> 00:32:59,224
第一層、第二層，一直加到第七層

693
00:32:59,224 --> 00:33:01,332
那發現說，假設現在

694
00:33:01,332 --> 00:33:05,386
那些 noise 是加在最後第七層的話

695
00:33:05,386 --> 00:33:07,141
對結果幾乎沒有影響

696
00:33:07,141 --> 00:33:09,217
下面這個是 noise 越加越大

697
00:33:09,217 --> 00:33:10,417
noise 越加越大

698
00:33:10,417 --> 00:33:13,573
對縱軸這個正確率，本來正確率是 [一百 percent] {100%}

699
00:33:13,573 --> 00:33:16,091
但是，你加一些 noise 幾乎沒什麼影響

700
00:33:16,091 --> 00:33:17,690
但是，如果你加在第一層

701
00:33:17,690 --> 00:33:20,854
一樣的 noise，加在第一層，整個結果就壞掉了

702
00:33:20,854 --> 00:33:23,066
整個結果就突然爆炸這樣子

703
00:33:23,066 --> 00:33:26,138
顯示說，第一層的 network 是非常 sensitive 的

704
00:33:26,138 --> 00:33:28,720
你只要稍微加一點 noise，他就會壞掉了

705
00:33:28,720 --> 00:33:33,433
另外這個實驗，如果沒記錯，應該是做在 MNIST 上面

706
00:33:33,433 --> 00:33:36,112
縱軸是正確率，這個實驗是這樣子的

707
00:33:36,112 --> 00:33:38,817
這個實驗是說，拿一個 network 出來

708
00:33:38,817 --> 00:33:41,129
我們只 train 某一個 layer

709
00:33:41,129 --> 00:33:43,564
其他 layer fix 住都是 random 的

710
00:33:43,564 --> 00:33:46,052
只 train 一個 layer，其他都 fix 住

711
00:33:46,052 --> 00:33:48,259
這邊的顏色跟這邊是一樣的

712
00:33:48,259 --> 00:33:50,567
所以，這個紫色就是這邊的紫色

713
00:33:50,567 --> 00:33:53,300
假設我們只 train 第一層

714
00:33:53,300 --> 00:33:56,746
第二層到最後一層通通是 error，通通是 random 的話

715
00:33:56,746 --> 00:33:59,986
其實你也可以得到大概 [九十 percent] {90%} 的正確率

716
00:33:59,986 --> 00:34:01,395
其實這沒有很高，因為 MNIST

717
00:34:01,395 --> 00:34:05,205
MNIST 胡亂做都是 [九十八 percent] {98%} 這樣

718
00:34:05,205 --> 00:34:07,561
所以 [九十 percent] {90%} 是很差這樣子，但是

719
00:34:07,561 --> 00:34:10,157
神奇的就是說，我只 learn 了第一層

720
00:34:10,157 --> 00:34:11,504
後面都是 random

721
00:34:11,504 --> 00:34:15,240
還是有 [九十 percent] {90%}，顯示第一層非常的重要

722
00:34:15,240 --> 00:34:16,666
但是，假設我們是說

723
00:34:16,666 --> 00:34:18,341
前面都是 random

724
00:34:18,341 --> 00:34:19,768
只 learn 最後一層

725
00:34:19,768 --> 00:34:21,278
結果就很爛這樣子

726
00:34:21,278 --> 00:34:22,547
所以這告訴我們說

727
00:34:22,547 --> 00:34:25,721
deep network 裡面，前面的 layer 是

728
00:34:25,721 --> 00:34:27,640
比較 sensitive，比較重要的

729
00:34:27,640 --> 00:34:31,862
我們在這邊休息 [十] {10} 分鐘，等一下再回來

730
00:34:51,973 --> 00:34:55,018
剛才講的是用 shallow 的 network

731
00:34:55,018 --> 00:34:56,633
來 fit 某一個 function

732
00:34:56,633 --> 00:35:00,506
剛才又講了說假設比較 deep 和 shallow network 的話

733
00:35:00,506 --> 00:35:03,565
deep 的 network 可以製造出比較多的片段

734
00:35:03,565 --> 00:35:05,142
但是，這並沒有完全的

735
00:35:05,167 --> 00:35:09,414
deep 的 network 可以製造出比較多片段這件事情

736
00:35:09,414 --> 00:35:12,837
跟我們要 fit 某一個 function

737
00:35:12,837 --> 00:35:15,254
並沒有直接的相關

738
00:35:15,254 --> 00:35:16,880
我們現在真正關心的問題是

739
00:35:16,880 --> 00:35:21,070
如果用 deep 的 structure 來 fit 某一個 function 的話

740
00:35:21,070 --> 00:35:23,504
會是什麼樣子？

741
00:35:23,504 --> 00:35:27,538
那我們假設我們現在要 fit 的 function 呢

742
00:35:27,538 --> 00:35:29,220
是一個 比較簡單的 function

743
00:35:29,220 --> 00:35:31,589
從這個比較簡單的 function 討論起

744
00:35:31,589 --> 00:35:36,177
這個 function 是 [f of x] {f(x)} [等於] {=} [x 平方] {x^2}

745
00:35:36,177 --> 00:35:38,299
他長得就是這個樣子

746
00:35:38,299 --> 00:35:41,045
[x 平方] {x^2} 在[零] {0}到[一] {1}區間呢

747
00:35:41,045 --> 00:35:42,787
長的是這個樣子

748
00:35:42,787 --> 00:35:45,270
現在就算是

749
00:35:45,270 --> 00:35:47,012
我們不管是用 shallow 的 structure

750
00:35:47,012 --> 00:35:48,606
還是 deep 的 structure

751
00:35:48,606 --> 00:35:50,030
我們製造出來的 function

752
00:35:50,030 --> 00:35:52,118
都是 piecewise linear 的

753
00:35:52,118 --> 00:35:55,243
所以在討論怎麼用一個 shallow 的 network 或者甚至是

754
00:35:55,243 --> 00:35:57,231
deep 的 network 來 fit 某一個 function 之前

755
00:35:57,231 --> 00:35:59,343
我們要討論第一件事情

756
00:35:59,343 --> 00:36:03,227
都是怎麼用一個 piecewise 的 linear 的 function

757
00:36:03,227 --> 00:36:05,829
去 fit 我們現在的 target function

758
00:36:05,829 --> 00:36:08,212
我們現在 target function 是 [x 平方] {x^2}

759
00:36:08,212 --> 00:36:11,611
怎麼用一個 piecewise linear function

760
00:36:11,611 --> 00:36:13,220
去 fit 這一個

761
00:36:13,220 --> 00:36:16,106
[f of x] {f(x)} [等於] {=} [x 平方] {x^2} 這個 function 呢

762
00:36:16,106 --> 00:36:18,866
我們現在定義另外一個 function 呢

763
00:36:18,866 --> 00:36:21,316
叫做 [f 小 m of x] {fm(x)}

764
00:36:21,316 --> 00:36:28,111
那這個 [f 小 m of x] {fm(x)} 他總共會有 [二的 m 次方] {2^m} 個片段

765
00:36:28,111 --> 00:36:31,797
那 [f one of x] {f1(x)} 長的是這個樣子

766
00:36:31,797 --> 00:36:33,102
長得是這樣

767
00:36:33,102 --> 00:36:35,948
這個 [f one] {f1} 怎麼來呢？你就把這個

768
00:36:35,948 --> 00:36:38,229
你就取 [零 . 五] {0.5} 的地方

769
00:36:38,229 --> 00:36:39,448
你就取 [零 . 五] {0.5} 的地方

770
00:36:39,448 --> 00:36:42,462
然後跟頭接起來

771
00:36:42,462 --> 00:36:45,432
跟尾巴接起來

772
00:36:45,432 --> 00:36:47,191
就得到兩個片段

773
00:36:47,191 --> 00:36:48,794
所以今天 [f one] {f1(x)} 呢

774
00:36:48,794 --> 00:36:52,113
總共有 [二的一次方] {2^1}，也就是兩個片段

775
00:36:52,113 --> 00:36:56,032
接下來 [f two] {f2(x)} 呢？[f two] {f2(x)} 有[四] {4}個片段

776
00:36:56,032 --> 00:36:57,749
這[四] {4}個片段怎麼產生呢

777
00:36:57,749 --> 00:37:02,038
你就把 x 軸，橫軸啊

778
00:37:02,038 --> 00:37:07,112
橫軸等於[零] {0}到 [四分之一] {1/4} 的地方接起來

779
00:37:07,112 --> 00:37:10,122
x 軸等於 [四分之一] {1/4} 到 [二分之一] {1/2} 的地方接起來

780
00:37:10,122 --> 00:37:13,249
[二分之一] {1/2} 到 [四分之三] {3/4} 的地方接起來

781
00:37:13,249 --> 00:37:14,926
[四分之三] {3/4} 到[一] {1}的地方接起來

782
00:37:14,926 --> 00:37:17,998
我知道你在台下其實很難看得清楚這樣

783
00:37:17,998 --> 00:37:20,491
但是畫出來就是這個樣子，我也沒有辦法這樣

784
00:37:20,491 --> 00:37:24,550
所以 [f two] {f2(x)} 他有[四] {4}個片段

785
00:37:24,550 --> 00:37:27,227
[二的二次方] {2^2}，總共[四] {4}個片段

786
00:37:27,227 --> 00:37:28,809
他是從[零] {0}到 [二分之一] {1/2}

787
00:37:30,371 --> 00:37:34,887
[零] {0} 到 [四分之一] {1/4}，[四分之一] {1/4} 到 [二分之一] {1/2}，[二分之一] {1/2} 到 [四分之三] {3/4}，[四分之三] {3/4} 到 [一] {1}

788
00:37:34,887 --> 00:37:36,854
總共四個片段

789
00:37:36,854 --> 00:37:38,189
那你會發現說其實

790
00:37:38,189 --> 00:37:41,876
[f two] {f2(x)} 其實跟 [x 平方] {x^2} 其實也滿接近的，所以

791
00:37:41,876 --> 00:37:44,189
你有點看不清楚 [f two] {f2(x)} 在哪裡

792
00:37:44,189 --> 00:37:46,258
哪如果你畫 [f 三] {f3(x)} 的話

793
00:37:46,258 --> 00:37:47,617
他就有八個片段

794
00:37:47,617 --> 00:37:50,092
畫 [f 四] {f4(x)} 的話，他就有 [十六] {16} 個片段

795
00:37:50,092 --> 00:37:51,041
以此類推

796
00:37:51,041 --> 00:37:53,235
接下來，我們問的問題是

797
00:37:53,235 --> 00:37:57,236
這個 m 的值到底要到多少的時候

798
00:37:57,236 --> 00:37:59,197
就是我們先給定這個 ε

799
00:37:59,197 --> 00:38:02,833
m 的值要到多少的時候

800
00:38:02,833 --> 00:38:05,944
我們才能夠讓 [f of x] {f(x)}

801
00:38:05,944 --> 00:38:09,652
跟 [f m of x] {fm(x)} 他們之間的最大的差

802
00:38:09,652 --> 00:38:12,770
小於等於 ε 這樣

803
00:38:12,770 --> 00:38:15,658
當然我們知道說，m 越多

804
00:38:15,658 --> 00:38:19,225
[f m] {fm(x)} 跟 [f] {f(x)} 之間的差異就越小

805
00:38:19,225 --> 00:38:21,941
你這邊畫越多的片段

806
00:38:21,941 --> 00:38:25,259
那你畫圖做出來的 [f m] {fm(x)} 跟 [f] {f(x)}

807
00:38:25,259 --> 00:38:27,406
也就是 [x 平方] {x^2}，他們就越接近

808
00:38:27,406 --> 00:38:30,285
但是，假設今天他們的差距啊

809
00:38:30,285 --> 00:38:32,622
不可以超過 ε，要小於等於 ε 的話

810
00:38:32,622 --> 00:38:35,712
這個 m 應該要有多少呢？

811
00:38:35,712 --> 00:38:40,111
你實際上算一下的話，這邊就省略掉計算過程

812
00:38:40,111 --> 00:38:43,369
你就回去怒算一波這樣子

813
00:38:43,369 --> 00:38:45,795
如果我算出來有錯你再告訴我這樣子

814
00:38:45,795 --> 00:38:47,914
你就回去怒算一波，你就知道說

815
00:38:47,914 --> 00:38:51,230
今天要如何讓，m 要多少

816
00:38:51,230 --> 00:38:53,604
才能夠小於等於 ε 呢？

817
00:38:53,604 --> 00:38:55,566
才能夠讓他們差距小於等於 ε 呢？

818
00:38:55,566 --> 00:39:02,174
這個 m 要大於等於 [負二分之一 log 二 ε 減一] {-1/2log2(ε)-1} 的時候

819
00:39:02,174 --> 00:39:05,597
這個 m 就會小於等於 ε

820
00:39:05,597 --> 00:39:07,818
你說怎麼算哦

821
00:39:07,818 --> 00:39:12,727
你就把 [f m] {fm(x)} 的式子列出來

822
00:39:12,727 --> 00:39:15,988
然後你就可以算他跟 [x 平方] {x^2} 的差距

823
00:39:15,988 --> 00:39:18,957
這個計算並不困難，他只是有點繁瑣

824
00:39:18,957 --> 00:39:21,272
然後，你就可以算出說

825
00:39:21,272 --> 00:39:24,800
怎麼樣可以讓他們最大的差距小於等於 ε

826
00:39:24,800 --> 00:39:27,752
總之，你算出來就是這個樣子

827
00:39:27,752 --> 00:39:29,107
你說，欸這邊有一個負號

828
00:39:29,107 --> 00:39:30,799
那是不是這一項是負的呢？不是啊

829
00:39:30,799 --> 00:39:32,170
ε 是小於[一] {1}的

830
00:39:32,170 --> 00:39:33,640
ε 是一個很小的值

831
00:39:33,640 --> 00:39:36,498
懂嗎？ε 是一個很小的值

832
00:39:36,498 --> 00:39:37,981
所以 {log 二 ε} {log2(ε)} 是負的

833
00:39:37,981 --> 00:39:40,820
負的東西乘上負的東西是正的

834
00:39:40,820 --> 00:39:42,282
然後，再減掉[一] {1}這樣

835
00:39:42,282 --> 00:39:44,513
ε 可能是一個很小的值

836
00:39:44,513 --> 00:39:47,570
比如說，[二的負七次方] {2^(-7)}

837
00:39:47,570 --> 00:39:49,782
log 這邊就是變成 [負七] {-7}

838
00:39:49,782 --> 00:39:51,554
[負七] {-7} [乘以] {*} [二分之一] {1/2} 變成正的

839
00:39:51,554 --> 00:39:53,910
所以 m 要大於等於這個數值

840
00:39:53,910 --> 00:39:56,387
才能夠讓它小於等於 ε

841
00:39:56,387 --> 00:39:57,929
當然今天 ε 的值

842
00:39:57,929 --> 00:40:01,503
越小，m 就要越大，這樣

843
00:40:01,503 --> 00:40:06,998
那 m 是這個樣子，需要多少個片段呢？

844
00:40:06,998 --> 00:40:08,823
那你就這邊取 [二的 m 次方] {2^m}

845
00:40:08,823 --> 00:40:11,441
這邊取 [二] {2} 的這一項次方

846
00:40:11,441 --> 00:40:16,409
所以，你今天需要的片段的數目

847
00:40:16,409 --> 00:40:22,795
至少要大於等於 [二分之一] {1/2} [根號 ε 分之一] {1/sqrt(ε)} 個片段

848
00:40:22,795 --> 00:40:24,401
這個值怎麼來的呢？

849
00:40:24,401 --> 00:40:28,066
你就是取 [二] {2} 的這個次方

850
00:40:28,066 --> 00:40:29,669
你就把這個東西呢

851
00:40:29,669 --> 00:40:35,309
放在指數項，[二] {2} 的這個次方

852
00:40:35,309 --> 00:40:37,550
就是 [二分之一] {1/2} [一除以根號 ε] {1/sqrt(ε)}

853
00:40:37,550 --> 00:40:44,771
講到這邊，假設你沒有跟上的話

854
00:40:44,771 --> 00:40:47,022
你只要知道說，現在有一個

855
00:40:47,022 --> 00:40:49,205
[x 平方] {x^2} 的 function

856
00:40:49,205 --> 00:40:52,377
我們今天，如果要用

857
00:40:52,377 --> 00:40:56,257
[二] {2} 的 m 次方個

858
00:40:56,257 --> 00:40:59,223
一樣寬的片段

859
00:40:59,223 --> 00:41:02,620
去 fit [x 平方] {x^2} 這個 function

860
00:41:02,620 --> 00:41:06,069
那我們的 [二的 m 次方] {2^m} 的數目

861
00:41:06,069 --> 00:41:11,618
一定要大於 [二分之一] {1/2} [一除以根號 ε] {1/sqrt(ε)} pieces

862
00:41:11,618 --> 00:41:14,078
那你會發現說，這個東西算起來

863
00:41:14,078 --> 00:41:18,945
其實是比剛才，我們在第一堂課裡面得到的結果

864
00:41:18,945 --> 00:41:22,471
[l 除以 ε] {l/ε} 還要小的，對不對？

865
00:41:22,471 --> 00:41:25,749
你想想看，這邊他是除 ε

866
00:41:25,749 --> 00:41:29,410
這邊是除根號 ε

867
00:41:29,410 --> 00:41:32,898
所以，如果你算他的 big O 的話，這一項

868
00:41:32,898 --> 00:41:35,160
是比較小的

869
00:41:35,160 --> 00:41:37,480
那這個結果其實也很合理的，因為

870
00:41:37,480 --> 00:41:40,088
因為在第一堂課我們討論的是 general 的 case 嘛

871
00:41:40,088 --> 00:41:41,107
任意的 function

872
00:41:41,107 --> 00:41:43,211
這邊我們只討論 [x 平方] {x^2}

873
00:41:43,211 --> 00:41:46,805
那你需要比較少的，根據 [x 平方] {x^2} 的特性

874
00:41:46,805 --> 00:41:48,771
所以，你需要比較少的 function

875
00:41:48,771 --> 00:41:51,204
你需要比較少的片段就可以

876
00:41:51,204 --> 00:41:54,286
fit 他，這個結果也是頗為合理的

877
00:41:54,286 --> 00:41:56,782
那現在，我們剛才講過說

878
00:41:56,782 --> 00:41:58,704
假設是一個 shallow network 的話

879
00:41:58,704 --> 00:42:01,883
你要兩個 ReLU

880
00:42:01,883 --> 00:42:03,228
才能夠產生一個 piece

881
00:42:03,228 --> 00:42:04,500
但其實更少一個

882
00:42:04,500 --> 00:42:06,242
有時候一個 ReLU 就可以產生一個 piece

883
00:42:06,242 --> 00:42:08,655
無論如何，你需要的 neuron 的數目

884
00:42:08,655 --> 00:42:12,185
就是 [big O of 一除以根號 ε] {O(1/sqrt(ε))}

885
00:42:12,185 --> 00:42:14,891
你需要至少這麼多的 neuron

886
00:42:14,891 --> 00:42:16,619
就假設你是 shallow 的 network

887
00:42:16,619 --> 00:42:18,112
你需要至少這麼多的 neuron

888
00:42:18,112 --> 00:42:22,570
才可以 fit [f of x] {f(x)} [等於] {=} [x 平方] {x^2}

889
00:42:22,570 --> 00:42:24,833
這個是 shallow 的狀況

890
00:42:24,833 --> 00:42:28,732
那我們來看一下 deep 的狀況

891
00:42:28,732 --> 00:42:31,107
我們來看一下 deep 的狀況

892
00:42:31,107 --> 00:42:33,638
deep 他的厲害的地方就是

893
00:42:33,638 --> 00:42:35,725
他要製造出這麼多 pieces

894
00:42:35,725 --> 00:42:38,435
其實她並不需要這麼多的 neuron

895
00:42:38,435 --> 00:42:42,179
如果我們要用 deep 的 network 來產生 [f of x] {f(x)}[等於] {=}[x 平方] {x^2}

896
00:42:42,179 --> 00:42:43,991
那要怎麼做呢？你看哦

897
00:42:43,991 --> 00:42:47,294
你要產生 [f one] {f1(x)} 的話

898
00:42:47,294 --> 00:42:48,741
你要產生 [f one of x] {f1(x)}

899
00:42:48,741 --> 00:42:51,120
你其實只需要把一條

900
00:42:51,120 --> 00:42:54,597
斜率是[一] {1}的斜線

901
00:42:54,597 --> 00:42:59,344
減掉這一個東西

902
00:42:59,344 --> 00:43:01,778
他就是 [f one] {f1(x)}

903
00:43:01,778 --> 00:43:04,399
這樣大家可以接受嗎？

904
00:43:15,446 --> 00:43:19,155
想想看哦，這個是，我們先把

905
00:43:19,155 --> 00:43:20,941
[x 平方] {x^2} 畫出來

906
00:43:20,941 --> 00:43:24,631
[x 平方] {x^2} 假設畫出來是這個樣子

907
00:43:24,631 --> 00:43:28,248
把 [f one] {f1(x)} 畫出來這樣

908
00:43:28,248 --> 00:43:30,412
假設 [f one] {f1(x)} 畫出來是這個樣子

909
00:43:30,412 --> 00:43:35,159
假設 [f one] {f1(x)} 畫出來是這個樣子

910
00:43:35,159 --> 00:43:37,591
換藍色好了，是這個樣子

911
00:43:37,591 --> 00:43:46,903
那 [f one] {f1(x)} 跟這一個斜率是[一] {1}的直線

912
00:43:46,903 --> 00:43:48,961
他們中間這邊差多少

913
00:43:48,961 --> 00:43:52,035
假設這邊是 [零 . 五] {0.5} 的話

914
00:43:52,035 --> 00:43:58,793
這邊的高是 [零 . 五的平方] {(0.5)^2} 是 [零 . 二五] {0.25}

915
00:43:58,793 --> 00:44:00,908
這邊的高

916
00:44:00,908 --> 00:44:02,737
這是一個斜率是[一] {1}的直線

917
00:44:02,737 --> 00:44:04,678
我知道我畫的很不標準，但是這邊的高

918
00:44:04,678 --> 00:44:09,947
是[零 . 五] {0.5}，所以這邊的差距是 [零 . 二五] {0.25}

919
00:44:09,947 --> 00:44:14,977
所以，你把這一個斜線

920
00:44:14,977 --> 00:44:18,605
減掉這中間的差

921
00:44:18,605 --> 00:44:21,257
這邊最寬最高的地方是 [零 . 二五] {0.25}

922
00:44:21,430 --> 00:44:22,903
後面會慢慢變小這樣

923
00:44:22,903 --> 00:44:25,037
這邊是 [零] {0}，他頭尾的地方都是 [零] {0}

924
00:44:25,037 --> 00:44:27,066
中間差距最大是 [零 . 五] {0.5}

925
00:44:27,066 --> 00:44:30,870
就會變成這個 [f one] {f1(x)}

926
00:44:30,870 --> 00:44:34,080
這一個 piecewise linear 的 function

927
00:44:34,080 --> 00:44:37,291
這樣大家可以接受嗎？所以其實你把

928
00:44:37,291 --> 00:44:40,314
這一個藍色的斜線

929
00:44:40,314 --> 00:44:42,438
減掉這一個三角形

930
00:44:42,438 --> 00:44:45,522
這個三角形就是這邊這一塊

931
00:44:45,522 --> 00:44:48,540
雖然看起來不像，但他就是啊

932
00:44:48,540 --> 00:44:52,716
減掉這一塊現在塗顏色的地方

933
00:44:52,716 --> 00:44:56,346
他就變成了 [f one] {f1(x)} 這樣子

934
00:44:56,346 --> 00:44:57,959
接下來怎麼

935
00:44:57,959 --> 00:45:01,136
所以我們現在可以用這兩個 function 相減

936
00:45:01,136 --> 00:45:02,666
製造出 [f one] {f1(x)}

937
00:45:02,666 --> 00:45:04,666
怎麼製造出 [f two] {f2(x)} 呢？

938
00:45:04,666 --> 00:45:07,954
你就再產生這樣子、這樣子

939
00:45:07,954 --> 00:45:11,172
有兩個鋸齒的形狀

940
00:45:11,172 --> 00:45:14,616
第一個鋸齒減在這邊

941
00:45:14,616 --> 00:45:18,499
第二個鋸齒減在這邊

942
00:45:18,499 --> 00:45:21,980
你就產生 [f two] {f2(x)} 了，這樣

943
00:45:21,980 --> 00:45:26,767
這樣 ok 嗎？就是

944
00:45:26,767 --> 00:45:29,955
講到這邊大家還有問題要問的嗎？

945
00:45:30,184 --> 00:45:32,343
你說

946
00:45:35,033 --> 00:45:33,675
這兩個三角形嗎？

947
00:45:36,268 --> 00:45:34,802
他們高度是一樣的

948
00:45:39,585 --> 00:45:43,747
你可以自己 check 一下他們高度是一樣的

949
00:45:43,747 --> 00:45:50,937
那這個，我們就不要畫圖好了，這個

950
00:45:50,992 --> 00:45:51,935
這個圖其實是很難畫的

951
00:45:51,935 --> 00:45:56,863
你如果不相信就回去 check 看看這樣子

952
00:45:56,863 --> 00:45:59,114
總之，你把這個斜線

953
00:45:59,114 --> 00:46:01,054
減掉第一個三角形

954
00:46:01,054 --> 00:46:03,364
你就得到藍色這條線

955
00:46:03,364 --> 00:46:08,531
然後再減掉第二個這樣子的，兩個鋸齒的三角形

956
00:46:08,531 --> 00:46:11,054
減掉兩個鋸齒的三角形

957
00:46:11,054 --> 00:46:13,371
這滑鼠很難指

958
00:46:13,371 --> 00:46:14,821
兩個鋸齒的三角形

959
00:46:14,821 --> 00:46:16,337
你就得到 [x two] {x2} 這樣

960
00:46:16,337 --> 00:46:18,625
你就得到 [f two] {f2(x)}

961
00:46:18,625 --> 00:46:22,593
所以今天呢，如果我們要

962
00:46:22,593 --> 00:46:25,143
產生，所以我們知道說

963
00:46:25,143 --> 00:46:27,816
這個一個斜線減掉這個就是 [f one] {f1(x)}

964
00:46:27,816 --> 00:46:30,462
一個斜線減掉這個，再減掉這個就是 [f two] {f2(x)}

965
00:46:30,462 --> 00:46:34,111
如果我們今天要產生 [f m] {fm(x)} 的話

966
00:46:34,111 --> 00:46:37,542
那我們知道 m 的值要大於等於這一項

967
00:46:37,542 --> 00:46:40,931
如果我們要產生 [f m] {fm(x)} 的話

968
00:46:40,931 --> 00:46:43,192
那我們做的事情就是把這條斜線

969
00:46:43,192 --> 00:46:45,777
減掉一個三角形，再減兩個三角形

970
00:46:45,777 --> 00:46:47,347
減四個三角形，減...

971
00:46:47,873 --> 00:46:51,390
對，沒錯，減一個三角形

972
00:46:51,390 --> 00:46:53,424
減掉一個三角形，兩個三角形

973
00:46:53,424 --> 00:46:54,937
四個三角形，八個三角形

974
00:46:54,937 --> 00:46:57,976
一直減下去，直到減到

975
00:46:57,976 --> 00:47:00,870
[二的 m 次方] {2^m} 個三角形這樣子

976
00:47:00,870 --> 00:47:02,849
那他們的高度你要考慮一下

977
00:47:02,849 --> 00:47:04,940
第一個三角形他的高度是 [四分之一] {1/4}

978
00:47:04,940 --> 00:47:07,237
兩個三角形的時候，高度是 [十六分之一] {1/16}

979
00:47:07,237 --> 00:47:11,371
[二的 m 次方] {2^m} 個三角形的時候，他的高度是 [四的 m 次方分之一] {1/(4^m)}

980
00:47:11,371 --> 00:47:15,784
接下來呢，你要做的事情就是

981
00:47:15,784 --> 00:47:19,185
產生這一連串的三角形這樣子

982
00:47:19,185 --> 00:47:21,356
那怎麼產生這一連串的三角形呢？

983
00:47:21,356 --> 00:47:24,263
其實，就跟我們在上一堂課講的

984
00:47:24,263 --> 00:47:26,168
我們不是說，用這個

985
00:47:26,168 --> 00:47:29,228
V 字型的，用這個

986
00:47:29,228 --> 00:47:31,617
取絕對值的 neuron 其實就是兩個 ReLU

987
00:47:31,617 --> 00:47:34,392
我們在第一層就可以製造出

988
00:47:34,392 --> 00:47:36,344
一個 v 的形狀

989
00:47:36,344 --> 00:47:39,388
再把 v 的形狀乘上一個負號，就是一個三角形了

990
00:47:39,388 --> 00:47:41,628
那第二層你可以製造出一個 w

991
00:47:41,628 --> 00:47:43,446
把 w 的形狀乘上一個負號

992
00:47:43,446 --> 00:47:46,371
就是變成一個 m 的形狀，就變成兩個三角形了

993
00:47:46,371 --> 00:47:50,668
一直到你總共有 m 層

994
00:47:50,668 --> 00:47:52,071
到第 m 層的時候

995
00:47:52,071 --> 00:47:55,979
你就可以製造這個有 [二的 m 次方] {2^m} 個三角形的鋸齒狀的圖

996
00:47:55,979 --> 00:48:00,034
所以今天，假設你要製造這些 function

997
00:48:00,034 --> 00:48:02,524
我們假設你要製造這些 function

998
00:48:02,524 --> 00:48:05,103
其實你只需要一個

999
00:48:05,103 --> 00:48:09,218
一個有 m 層的 ReLU 的 network

1000
00:48:09,218 --> 00:48:10,685
其實就可以辦到了

1001
00:48:10,685 --> 00:48:13,971
所以，如果我今天要製造這個 [f m] {fm(x)}

1002
00:48:13,971 --> 00:48:17,636
其實我只需要產生這個東西，那這個東西很簡單

1003
00:48:17,636 --> 00:48:19,611
even 不需要 activation function 就可以製造

1004
00:48:19,611 --> 00:48:20,632
就 input [等於] {=} output 嘛

1005
00:48:20,632 --> 00:48:22,459
就 input [等於] {=} output，就原來的 x

1006
00:48:22,459 --> 00:48:25,986
然後再減掉 [a one] {a1}

1007
00:48:25,986 --> 00:48:28,043
再減掉 [a two] {a2}

1008
00:48:28,043 --> 00:48:31,785
再一直減到 [a m] {am}，只是你要稍微註明一下這中間的 scalar

1009
00:48:31,785 --> 00:48:34,295
你其實就製造出 [f m] {fm(x)} 了

1010
00:48:34,295 --> 00:48:36,834
所以，假設你要製造 [f m] {fm(x)} 的話

1011
00:48:36,834 --> 00:48:40,000
你只需要 m 個 layer

1012
00:48:40,000 --> 00:48:43,416
你只需要 [big O of m] {O(m)} 個 neuron

1013
00:48:43,416 --> 00:48:46,104
然後，總之 [big O of m] {O(m)} 個 layer

1014
00:48:46,104 --> 00:48:48,442
就可以製造這種 activation function

1015
00:48:48,442 --> 00:48:53,315
如果你把這個 m 代 [負二分之一 log 二 ε 減一] {-1/2 log2(ε) - 1}的話

1016
00:48:53,315 --> 00:48:56,245
那這個 [負二分之一] {-1/2}

1017
00:48:56,245 --> 00:48:59,230
這個可以提到 log 裡面啦，變成

1018
00:48:59,230 --> 00:49:01,164
[一除以根號 ε] {1/sqrt(ε)}

1019
00:49:01,164 --> 00:49:04,187
所以你只需要 [log 二一除以根號 ε] {log2(1/sqrt(ε))} 個 neuron

1020
00:49:04,187 --> 00:49:07,406
然後 [log 二一除以根號 ε] {log2(1/sqrt(ε))} 個 layer

1021
00:49:07,406 --> 00:49:10,575
其實就可以產生、就可以去逼近

1022
00:49:10,575 --> 00:49:13,520
y [等於] {=} [x 平方] {x^2} 這樣的 target function

1023
00:49:13,520 --> 00:49:17,065
那你可能會想說，好像只討論了

1024
00:49:17,540 --> 00:49:19,856
就假設剛才的東西你都沒跟上的話，你就只要知道說

1025
00:49:19,856 --> 00:49:22,105
如果我們今天用 deep 的 network

1026
00:49:22,105 --> 00:49:25,013
因為 deep 的 network 可以輕易地產生這種鋸齒的形狀

1027
00:49:25,013 --> 00:49:28,636
所以我們就可以輕易地 fit y [等於] {=} [x 平方] {x^2}

1028
00:49:28,636 --> 00:49:31,137
比 general 的，用 shallow 的 network

1029
00:49:31,137 --> 00:49:33,580
還要的 neuron 少很多

1030
00:49:33,580 --> 00:49:37,712
那為什麼我們在意 y [等於] {=} [x 平方] {x^2}

1031
00:49:37,712 --> 00:49:42,324
你可能覺得說只考慮 y [等於] {=} [x 平方] {x^2} 非常的 limited

1032
00:49:42,324 --> 00:49:44,370
但其實不會， y [等於] {=} [x 平方] {x^2}

1033
00:49:44,370 --> 00:49:45,939
他有很多的妙用

1034
00:49:45,939 --> 00:49:47,627
怎麼用 y [等於] {=} [x 平方] {x^2} 呢？

1035
00:49:47,627 --> 00:49:49,312
我們現在知道說

1036
00:49:49,312 --> 00:49:55,437
我們只要 [big O log 二一除以根號 ε] {O(log2(1/sqrt(ε)))} 個 neuron

1037
00:49:55,437 --> 00:49:57,029
我們就可以製造一個 Square Net

1038
00:49:57,029 --> 00:49:58,561
這個 Square Net 他做的事情

1039
00:49:58,561 --> 00:50:00,762
不像現在 R Net 都做一些很複雜的事情

1040
00:50:00,762 --> 00:50:01,786
他就是乘平方

1041
00:50:01,786 --> 00:50:04,929
然後他的誤差會小於等於 ε

1042
00:50:04,929 --> 00:50:08,484
那我們能夠製造 Square Net 以後

1043
00:50:08,484 --> 00:50:10,689
我們就可以製造一種特殊的 network 叫做

1044
00:50:10,689 --> 00:50:12,299
Multiply Net

1045
00:50:12,299 --> 00:50:15,238
他 Multiply Net 做的事情就是給他 [x one] {x1}, [x two] {x2}

1046
00:50:15,238 --> 00:50:17,781
他給你 output 把 [x one] {x1}, [x two] {x2} 相乘

1047
00:50:17,781 --> 00:50:19,367
怎麼從 Square Net

1048
00:50:19,367 --> 00:50:21,367
製造 Multiply Net 呢？

1049
00:50:21,367 --> 00:50:27,958
因為 y [等於] {=} [x one] {x1}[x two] {x2} [等於] {=} [二分之一] {1/2} [x one 加 x two 的平方 減 x one 平方 減 x two 平方] {((x1 + x2)^2 - x1^2 - x2^2)}

1050
00:50:27,958 --> 00:50:30,664
你只要把 [x one] {x1}, [x two] {x2} 的平方展開

1051
00:50:30,664 --> 00:50:33,110
減 [x one] {x1} 平方、減 [x two] {x2} 平方，再乘以 [二分之一] {1/2}

1052
00:50:33,110 --> 00:50:34,980
那你就得到了 [x one] {x1}, [x two] {x2}

1053
00:50:34,980 --> 00:50:37,941
所以，今天我們只要會做 Square Net

1054
00:50:37,941 --> 00:50:40,975
你接下來就可以用三個 Square Net

1055
00:50:40,975 --> 00:50:43,954
拼出一個 Multiply Net

1056
00:50:43,954 --> 00:50:47,675
怎麼拼呢？我們先把 [x one] {x1}, [x two] {x2} 加起來

1057
00:50:47,675 --> 00:50:49,526
丟到 Square Net 裡面

1058
00:50:49,526 --> 00:50:52,952
然後把 [x one] {x1} 獨自丟到 Square Net 裡面

1059
00:50:52,952 --> 00:50:56,111
把 [x two] {x2} 獨自丟到 Square Net 裡面

1060
00:50:56,111 --> 00:50:59,028
然後這邊就是算出 [x one 加 x two 的平方] {([x1 + x2)^2}

1061
00:50:59,028 --> 00:51:01,902
這邊算出 [x two 的平方] {x2^2}

1062
00:51:01,902 --> 00:51:03,645
這邊算出 [x one 平方] {x1^2}

1063
00:51:03,645 --> 00:51:05,968
這三項就是這邊的這三項

1064
00:51:05,968 --> 00:51:08,168
把它算出來都乘上 [二分之一] {1/2}

1065
00:51:08,168 --> 00:51:10,578
加起來就得到 [x one] {x1} [x two] {x2}

1066
00:51:10,578 --> 00:51:13,165
所以，今天我們能夠用

1067
00:51:13,165 --> 00:51:15,902
這麼多的 neuron 做出 Square Net

1068
00:51:15,902 --> 00:51:20,121
我們就是用三倍的 neuron 就可以做出 Multiply Net 了

1069
00:51:20,121 --> 00:51:24,858
那 [big O] {O} 的這個 complexity 是不變的

1070
00:51:24,858 --> 00:51:27,128
因為你只是在前面乘上一個常數而已

1071
00:51:27,128 --> 00:51:30,043
能夠做 Multiply Net 以後呢？

1072
00:51:30,043 --> 00:51:32,741
接下來你就可以做 Polynomial

1073
00:51:32,741 --> 00:51:37,089
因為你就可以做，至少你可以先做 y [等於] {=} [x n 方] {x^n}

1074
00:51:37,089 --> 00:51:38,649
怎麼做 y [等於] {=} [x n 方] {x^n} 呢？

1075
00:51:38,649 --> 00:51:42,248
其實很簡單，你就用一個 Square Net，把 x 變成 [x 平方] {x^2}

1076
00:51:42,248 --> 00:51:44,123
接下來我們會做 Multiply Net

1077
00:51:44,123 --> 00:51:47,018
你就會把 [x 平方] {x^2} 跟 x 乘起來變成 [x 三方] {x^3}

1078
00:51:47,018 --> 00:51:50,496
你還可以把 [x 三方] {x^3} 跟 x 乘起來就變成 [x 四方] {x^4}

1079
00:51:50,496 --> 00:51:52,586
所以今天要產生 [x n 方] {x^n}

1080
00:51:52,586 --> 00:51:55,870
沒有問題，你可以用一堆的 Multiply Net

1081
00:51:55,870 --> 00:51:58,079
就可以做到這件事情

1082
00:51:58,079 --> 00:51:59,045
但這不是唯一的方法

1083
00:51:59,045 --> 00:52:01,039
你永遠可以想一下別的方法，舉例來說

1084
00:52:01,039 --> 00:52:04,793
你也可以只用 Square Net 就算出 [x 四方] {x^4} 等等

1085
00:52:04,793 --> 00:52:10,338
總之你有 Square Net、Multiply Net，你就可以算 y [等於] {=} [x n 方] {x^n}

1086
00:52:10,338 --> 00:52:12,954
而這邊的每一個 block

1087
00:52:12,954 --> 00:52:16,177
需要的 neuron 的 complexity 啊

1088
00:52:16,177 --> 00:52:19,533
是這麼多，是 [log 二一除以根號 ε] {log2(1/sqrt(ε))}

1089
00:52:19,533 --> 00:52:22,366
那接下來，你可以算 y [等於] {=} [x 平方] {x^2}

1090
00:52:22,366 --> 00:52:25,198
你就製造一個東西叫做 Power Net

1091
00:52:25,198 --> 00:52:26,243
這後面有一個 n

1092
00:52:26,243 --> 00:52:27,843
就是他可以把 input 的 x

1093
00:52:27,843 --> 00:52:31,417
乘一乘就變成 [x n 方] {x^n}，就叫他 Power(n) Net

1094
00:52:31,417 --> 00:52:33,842
你有 Power Net 以後呢，你就可以

1095
00:52:33,842 --> 00:52:35,516
就可以產生 Polynomial 了

1096
00:52:35,516 --> 00:52:37,599
你就可以產生 Polynomial，怎麼就產生 Polynomial

1097
00:52:37,599 --> 00:52:40,636
你用 [power n] {Power(n)} Net 產生 [x n 方] {x^n}

1098
00:52:40,636 --> 00:52:41,958
前面再乘 [a n] {an}

1099
00:52:41,958 --> 00:52:45,869
然後，你用 [Power n 減一] {Power(n-1)} Net，產生 [x n 減一次方] {x^(n-1)}

1100
00:52:45,869 --> 00:52:47,279
前面再乘 [a n 減一] {a(n-1)}

1101
00:52:47,279 --> 00:52:48,604
把他們通通加起來

1102
00:52:48,604 --> 00:52:52,369
你就可以產生任何你想要的 Polynomial 了

1103
00:52:52,369 --> 00:52:55,347
那你說 Polynomial 不夠 general

1104
00:52:55,347 --> 00:52:56,624
其實  Polynomial 就夠 general 了

1105
00:52:56,624 --> 00:53:00,210
你就可以用 Polynomial 去 fit 其他 continuous function

1106
00:53:00,210 --> 00:53:01,039
就結束了

1107
00:53:01,039 --> 00:53:03,078
所以，我們現在可以用 deep structure

1108
00:53:03,078 --> 00:53:05,336
我們會做 [x 平方] {x^2}

1109
00:53:05,336 --> 00:53:11,036
[x 平方] {x^2} 只要 [big O of log 一除以 ε] {O(log2(1/ε))} 個 neuron

1110
00:53:11,036 --> 00:53:14,017
接下來，最後就可以產生 Polynomial

1111
00:53:14,017 --> 00:53:15,622
我們就可以用 Polynomial 的 function

1112
00:53:15,622 --> 00:53:16,943
去 fit 其他的 function

1113
00:53:16,943 --> 00:53:19,148
我們就知道說，怎麼用 deep network

1114
00:53:19,148 --> 00:53:22,231
的架構去 fit 其他的 function

1115
00:53:22,231 --> 00:53:24,219
當然，你可能會說

1116
00:53:24,219 --> 00:53:26,464
在實作上，那個 network 不是這樣

1117
00:53:26,464 --> 00:53:29,141
這個 network 的參數不是像你這樣手設出來的啊

1118
00:53:29,141 --> 00:53:30,516
那個是 learn 出來的啊

1119
00:53:30,516 --> 00:53:32,683
我們現在還沒有討論那個問題

1120
00:53:32,683 --> 00:53:33,925
我們只是想一下，討論說

1121
00:53:33,925 --> 00:53:36,508
假設你要 fit 某一個 function，有沒有辦法做到

1122
00:53:36,508 --> 00:53:38,118
實際上你找不找得到

1123
00:53:38,118 --> 00:53:40,037
那個 function，那是 optimization 問題

1124
00:53:40,037 --> 00:53:41,755
那是我們之後才要討論的問題

1125
00:53:41,755 --> 00:53:43,215
不是我們今天要討論的問題

1126
00:53:43,215 --> 00:53:45,710
所以，我們現在得到這樣的結果

1127
00:53:45,710 --> 00:53:49,335
我們要 fit y [等於] {=} [x 平方] {x^2}

1128
00:53:49,335 --> 00:53:51,510
如果是 shallow 的 network

1129
00:53:51,510 --> 00:53:54,912
你的 neuron 的數目是 [big O of 一除以根號 ε] {O(1/sqrt(ε))}

1130
00:53:54,912 --> 00:53:57,357
那我們就把 [一除以根號 ε] {1/sqrt(ε)} 畫出來

1131
00:53:57,357 --> 00:53:58,372
橫軸是 ε

1132
00:53:58,372 --> 00:54:02,004
然後把 [一除以根號 ε] {1/sqrt(ε)} 的線畫出來

1133
00:54:02,004 --> 00:54:05,895
那 deep network 是 [log 一除以根號 ε] {log(1/sqrt(ε))}

1134
00:54:05,895 --> 00:54:08,265
所以你發現說，他們中間的差距

1135
00:54:08,265 --> 00:54:10,392
是有一個 log 項

1136
00:54:10,392 --> 00:54:13,732
他們中間的差距有 exponential 那麼多

1137
00:54:13,732 --> 00:54:15,606
他們有 exponential 的差距

1138
00:54:15,606 --> 00:54:21,051
deep 和 shallow，你要達到同樣的 error 的時候

1139
00:54:21,051 --> 00:54:22,654
同樣的這個 error 的時候

1140
00:54:22,654 --> 00:54:25,219
deep 他需要的 neuron

1141
00:54:25,219 --> 00:54:28,614
是 shallow 需要的 neuron 再取 log

1142
00:54:28,614 --> 00:54:32,666
或者是說，deep 可以用某個數量的 neuron

1143
00:54:32,666 --> 00:54:34,246
達成某個 accuracy

1144
00:54:34,246 --> 00:54:37,173
那 shallow 的 network 要 exponential 多的 neuron

1145
00:54:37,173 --> 00:54:38,961
才可以達到那個 accuracy

1146
00:54:38,961 --> 00:54:40,670
但是，這樣子的討論呢

1147
00:54:40,670 --> 00:54:42,363
是不足夠的

1148
00:54:42,363 --> 00:54:44,796
你覺得不足在哪裡呢？

1149
00:54:44,796 --> 00:54:47,754
你仔細想想看

1150
00:54:47,754 --> 00:54:51,525
你回憶一下列人第 [二十] {20} 集，比司吉告訴我們的

1151
00:54:51,525 --> 00:54:53,659
比司吉說，假設有很多的

1152
00:54:53,659 --> 00:54:57,628
每個人的能力範圍其實都是一個 range

1153
00:54:57,628 --> 00:55:01,576
今天我們拿 C 跟 A 來比較

1154
00:55:01,576 --> 00:55:03,975
然後 A，我們找他的一個 case

1155
00:55:03,975 --> 00:55:07,094
正好找到他，比如說，狀況特別差的一個 case

1156
00:55:07,094 --> 00:55:08,490
Ｃ我們找到一個 case

1157
00:55:08,490 --> 00:55:10,374
正好找到一個狀況特別好的 case

1158
00:55:10,374 --> 00:55:12,303
我們就會覺得說 C 可以贏過 A

1159
00:55:12,303 --> 00:55:15,693
但其實，那只是因為 A 正好選到狀況特別差的 case

1160
00:55:15,693 --> 00:55:17,875
也許 A 在最佳狀態的時候

1161
00:55:17,875 --> 00:55:19,467
他是可以打爆 C 的

1162
00:55:19,467 --> 00:55:20,977
所以今天的狀況也是一樣

1163
00:55:20,977 --> 00:55:22,535
我們剛才說 shallow

1164
00:55:22,535 --> 00:55:24,716
需要這麼多的 neuron 只是說

1165
00:55:24,716 --> 00:55:26,990
我們想了一個方法需要這麼多的 neuron

1166
00:55:26,990 --> 00:55:29,205
並不代表說那是最佳的 solution 啊

1167
00:55:29,205 --> 00:55:30,684
對不對？我們只是說

1168
00:55:30,684 --> 00:55:32,900
如果有這麼多 neuron

1169
00:55:32,900 --> 00:55:35,304
我可以 fit y [等於] {=} [x 平方] {x^2}

1170
00:55:35,304 --> 00:55:37,544
但是並不代表說，我一定要那麼多

1171
00:55:37,544 --> 00:55:39,639
才能 fit y [等於] {=} [x 平方] {x^2}，也許需要

1172
00:55:39,639 --> 00:55:41,145
也許只要比較少的 neuron

1173
00:55:41,145 --> 00:55:42,382
就可以辦到這件事情

1174
00:55:42,382 --> 00:55:43,639
也說不定，也許

1175
00:55:43,639 --> 00:55:46,206
這是 shallow network 他在很糟的狀態

1176
00:55:46,206 --> 00:55:49,115
他其實是 A，他在這邊，在很糟的狀態

1177
00:55:49,115 --> 00:55:51,710
也許他竭盡全力的時候是這個樣子

1178
00:55:51,710 --> 00:55:52,912
他可以打爆 deep

1179
00:55:52,912 --> 00:55:55,355
只是我們不知道而已

1180
00:55:55,355 --> 00:55:58,867
所以，接下來要問的下一個問題就是

1181
00:55:58,867 --> 00:56:00,249
假設我們讓

1182
00:56:00,249 --> 00:56:01,708
所以下一段要講的就是

1183
00:56:01,708 --> 00:56:04,100
假設我們現在讓 shallow 的 network

1184
00:56:04,100 --> 00:56:08,606
竭盡全力，他能不能夠打爆 deep 呢？

